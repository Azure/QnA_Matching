{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document (QnA) Matching Data Science Process\n",
    "\n",
    "## Part 1: Text Preparation and Phrases Learning\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook is Part 1 of 5, in a series providing a step-by-step description of how to create discriminative training methods to match the correct answer to a given question. Using Python packages and custom code examples, we have implemented the basic framework that combines key phrase learning and latent topic modeling as described in the paper entitled [\"Modeling Multiword Phrases with Constrained Phrases Tree for Improved Topic Modeling of Conversational Speech\"](http://people.csail.mit.edu/hazen/publications/Hazen-SLT-2012.pdf) which was originally presented in the 2012 IEEE Workshop on Spoken Language Technology.\n",
    "\n",
    "Although the paper examines the use of the technology for analyzing human-to-human conversations, the techniques are quite general and can be applied to a wide range of natural language data including news stories, legal documents, research publications, social media forum discussions, customer feedback forms, product reviews, and many more.\n",
    "\n",
    "Also, we implement a Naive Bayes Classifier as described in the paper entitled [\"MCE Training Techniques for Topic Identification of Spoken Audio Documents\"](http://ieeexplore.ieee.org/abstract/document/5742980/).\n",
    "\n",
    "Part 1 of the series shows how to pre-process the text data, learn the most salient phrases present in a large collection of documents and save cleaned text data in the Azure Blob Storage. These phrases can be treated as single compound word units in down-stream processes such as discriminative training.\n",
    "\n",
    "Note: This notebook series are built under Python 3.5 and NLTK 3.2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required Python modules\n",
    "\n",
    "In this notebook, we use several open-source Python packages that need to be installed in a local machine or an Azure Notebook Server. An upgrade is requested if a previous version of a package has been installed in the past.\n",
    "\n",
    "We make use of the NLTK sentence tokenization capability which takes a long string of text and splits it into sentence units. The tokenizer requires the installation of the 'punkt' tokenizer models. After importing nltk, the nltk.download() function can be used to download specific packages such as 'punkt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uncomment the below code to install/upgrade the requested Python packages.\n",
    "# !pip install --upgrade --no-deps smart_open azure pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import requests\n",
    "import re\n",
    "import itertools\n",
    "import nltk\n",
    "import math\n",
    "import numpy as np\n",
    "import csv \n",
    "import gc\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from collections import (namedtuple, Counter)\n",
    "from azure.storage import CloudStorageAccount\n",
    "from IPython.display import display\n",
    "\n",
    "# suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "EMPTY = ''\n",
    "SPACE = ' '\n",
    "nltk.download(\"punkt\")\n",
    "NLTK_PUNKT_EN = 'tokenizers/punkt/english.pickle'\n",
    "SENTENCE_BREAKER = nltk.data.load(NLTK_PUNKT_EN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure blob storage\n",
    "\n",
    "Configure Azure Blob Storage to retrieve and store datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "storage_account_name = 'mezsa'\n",
    "storage_account_key = 'X1Xwyn5ROxyQa4tmvjSza/Lv5bXLu7cZ1jWyfFhCEBCKFr78onDgFUH05F5iG2aq1IsU+DIooYDbPzKa821FSA=='\n",
    "account = CloudStorageAccount(account_name=storage_account_name, account_key=storage_account_key)\n",
    "blob_service = account.create_blob_service()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access sample data\n",
    "\n",
    "We used three sets of data in this series of notebooks. We obtain the raw data from Stack Overflow Database and extract all question-answer pairs related to the \"JavaScript\" tag. For the question-answer pairs, we consider the following scenarios. \n",
    "\n",
    "1. Original Questions (Q): These questions have been asked and answered on the Stack Overflow.\n",
    "2. Duplications (D): There is a linkage among questions. Some questions that have already been asked by others are linked to the previous/original questions as Duplications. In the Stack Overflow Database, this kind of linkage is determined by \"LINK_TYPE_DUPE = 3\". Each original question could have 0 to many duplications.\n",
    "3. Answers (A): For each Original question and its Duplications, we found more than one answers resolved that question. In our analysis, we only select the Accepted answer or the answer with the highest score that resolved the Original question. Therefore, it's 1-to-1 mapping between Original questions and Answers and many-to-1 mapping between Duplications and Original questions. Each Original question and its Duplications has an unique AnswerId.\n",
    "4. Function Words: we consider a list of words that can only be used in between content words in the creation of phrases. This list of words are also used as Stop Words.\n",
    "\n",
    "See the below Data Diagram:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/data_diagram.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# functions to load .tsv.gz file into Pandas data frame.\n",
    "def read_csv_gz(url, **kwargs):\n",
    "    return pd.read_csv(gzip.open(requests.get(url, stream=True).raw, mode='rb'), **kwargs)\n",
    "\n",
    "def read_data_frame(url, **kwargs):\n",
    "    return read_csv_gz(url, sep='\\t', encoding='utf8', **kwargs).set_index('Id')\n",
    "\n",
    "# functions to load .txt file into a Python dictionary. \n",
    "def load_from_url(fileURL):\n",
    "    response = requests.get(fileURL, stream=True)\n",
    "    return response.text.split('\\n')\n",
    "\n",
    "def LoadListAsHash(fileURL):\n",
    "    listHash = {}\n",
    "    wordsList = load_from_url(fileURL)\n",
    "    # Read in lines one by one stripping away extra spaces, \n",
    "    # leading spaces, and trailing spaces and inserting each\n",
    "    # cleaned up line into a hash table\n",
    "    re1 = re.compile(' +')\n",
    "    re2 = re.compile('^ +| +$')\n",
    "    for stringIn in wordsList:\n",
    "        term = re2.sub(\"\",re1.sub(\" \",stringIn.strip('\\n')))\n",
    "        if term != '':\n",
    "            listHash[term] = 1\n",
    "    return listHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# URLs to Original questions, Duplications, Answers and Function Words.\n",
    "questions_url = 'https://mezsa.blob.core.windows.net/stackoverflow/orig-q.tsv.gz'\n",
    "dupes_url = 'https://mezsa.blob.core.windows.net/stackoverflow/dup-q.tsv.gz'\n",
    "answers_url = 'https://mezsa.blob.core.windows.net/stackoverflow/ans.tsv.gz'\n",
    "function_words_url = 'https://mezsa.blob.core.windows.net/stackoverflow/function_words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load datasets.\n",
    "questions = read_data_frame(questions_url, names=('Id', 'AnswerId', 'Text0', 'CreationDate'))\n",
    "dupes = read_data_frame(dupes_url, names=('Id', 'AnswerId', 'Text0', 'CreationDate'))\n",
    "answers = read_data_frame(answers_url, names=('Id', 'Text0'))\n",
    "# Load the list of non-content bearing function words\n",
    "functionwordHash = LoadListAsHash(function_words_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AnswerId</th>\n",
       "      <th>Text0</th>\n",
       "      <th>CreationDate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220231</th>\n",
       "      <td>220233</td>\n",
       "      <td>Accessing the web page's HTTP Headers in JavaS...</td>\n",
       "      <td>2008-10-20 22:54:38.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391979</th>\n",
       "      <td>810461</td>\n",
       "      <td>Get client IP using just JavaScript?. &lt;p&gt;I nee...</td>\n",
       "      <td>2008-12-24 18:22:30.780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        AnswerId                                              Text0  \\\n",
       "Id                                                                    \n",
       "220231    220233  Accessing the web page's HTTP Headers in JavaS...   \n",
       "391979    810461  Get client IP using just JavaScript?. <p>I nee...   \n",
       "\n",
       "                   CreationDate  \n",
       "Id                               \n",
       "220231  2008-10-20 22:54:38.767  \n",
       "391979  2008-12-24 18:22:30.780  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examples of Original questions.\n",
    "questions.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing\n",
    "\n",
    "### Clean up text\n",
    "\n",
    "Since the raw data is in HTML format, we need to clean up HTML tags and links. We also remove embeded code chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strip_code(text):\n",
    "    if not isinstance(text, str): return text\n",
    "    return re.sub('<pre><code>.*?</code></pre>', EMPTY, text)\n",
    "\n",
    "def strip_tags(text):\n",
    "    if not isinstance(text, str): return text\n",
    "    return re.sub('<[^>]+>', EMPTY, text)\n",
    "\n",
    "def strip_links(text):\n",
    "    if not isinstance(text, str): return text\n",
    "    def replace_link(match):\n",
    "        return EMPTY if re.match('[a-z]+://', match.group(1)) else match.group(1)\n",
    "    return re.sub('<a[^>]+>(.*)</a>', replace_link, text)\n",
    "\n",
    "def clean_text(text):\n",
    "    return strip_tags(strip_links(strip_code(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for df in (questions, dupes, answers):\n",
    "    df['Text'] = df['Text0'].apply(clean_text).str.lower()\n",
    "    df['NumChars'] = df['Text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AnswerId</th>\n",
       "      <th>Text0</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Text</th>\n",
       "      <th>NumChars</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220231</th>\n",
       "      <td>220233</td>\n",
       "      <td>Accessing the web page's HTTP Headers in JavaS...</td>\n",
       "      <td>2008-10-20 22:54:38.767</td>\n",
       "      <td>accessing the web page's http headers in javas...</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391979</th>\n",
       "      <td>810461</td>\n",
       "      <td>Get client IP using just JavaScript?. &lt;p&gt;I nee...</td>\n",
       "      <td>2008-12-24 18:22:30.780</td>\n",
       "      <td>get client ip using just javascript?. i need t...</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        AnswerId                                              Text0  \\\n",
       "Id                                                                    \n",
       "220231    220233  Accessing the web page's HTTP Headers in JavaS...   \n",
       "391979    810461  Get client IP using just JavaScript?. <p>I nee...   \n",
       "\n",
       "                   CreationDate  \\\n",
       "Id                                \n",
       "220231  2008-10-20 22:54:38.767   \n",
       "391979  2008-12-24 18:22:30.780   \n",
       "\n",
       "                                                     Text  NumChars  \n",
       "Id                                                                   \n",
       "220231  accessing the web page's http headers in javas...       284  \n",
       "391979  get client ip using just javascript?. i need t...       288  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examples after the cleaning.\n",
    "questions.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set data selection criteria\n",
    "\n",
    "To obtain high quality datasets for learning phrases, we set a threshold of minimum length of characters in the text field. This threshold is considered seperatedly for Original questions, Duplications and Answers. \n",
    "\n",
    "For each Original question, we also make sure there are at least 3 linked Duplications so that we have enough data to learn from in the later Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a function to find the AnswerIds has at least 3 dupes.\n",
    "def find_answerId(answersC, dupesC, num_dupes):\n",
    "       \n",
    "    countHash = {}\n",
    "    for i in dupesC.AnswerId:\n",
    "        if i not in answersC.index.values:\n",
    "            continue\n",
    "        if i not in countHash.keys():\n",
    "            countHash[i] = 1\n",
    "        else:\n",
    "            countHash[i] += 1\n",
    "            \n",
    "    countHash = {k: v for k, v in countHash.items() if v >= num_dupes}\n",
    "    commonAnswerId = countHash.keys()\n",
    "    \n",
    "    return commonAnswerId\n",
    "\n",
    "# a function to extract data based on the selection criteria.\n",
    "def select_data(questions, dupes, answers):\n",
    "    # exclude the records without any text\n",
    "    questions_nz = questions.query('NumChars > 0')\n",
    "    dupes_nz = dupes.query('NumChars > 0')\n",
    "    answers_nz = answers.query('NumChars > 0')\n",
    "\n",
    "    # get the 10th percentile of text length as the minimum length of characters to consider in the text field\n",
    "    minLenQ = questions_nz.quantile(.1)['NumChars']\n",
    "    minLenD = dupes_nz.quantile(.1)['NumChars']\n",
    "    minLenA = answers_nz.quantile(.1)['NumChars']\n",
    "    \n",
    "    # eliminate records with text less than the minimum length\n",
    "    questionsC = questions.query('NumChars >' + str(int(minLenQ)))\n",
    "    dupesC = dupes.query('NumChars >' + str(minLenD))\n",
    "    answersC = answers.query('NumChars >' + str(minLenA))\n",
    "    \n",
    "    # make sure Questions 1:1 match with Answers \n",
    "    matches = questionsC.merge(answersC, left_on = 'AnswerId', right_index = True)\n",
    "    questionsC = matches[['AnswerId', 'Text0_x', 'CreationDate', 'Text_x', 'NumChars_x']]\n",
    "    questionsC.columns = ['AnswerId', 'Text0', 'CreationDate', 'Text', 'NumChars']\n",
    "\n",
    "    answersC = matches[['Text0_y', 'Text_y', 'NumChars_y']]\n",
    "    answersC.index = matches['AnswerId']\n",
    "    answersC.columns = ['Text0', 'Text', 'NumChars']\n",
    "    \n",
    "    # find the AnswerIds has at least 3 dupes\n",
    "    commonAnswerId = find_answerId(answersC, dupesC, 3)\n",
    "    \n",
    "    # select the records with those AnswerIds\n",
    "    questionsC = questionsC.loc[questionsC.AnswerId.isin(commonAnswerId)]\n",
    "    dupesC = dupesC.loc[dupesC.AnswerId.isin(commonAnswerId)]\n",
    "    answersC = answersC.loc[commonAnswerId] \n",
    "    \n",
    "    return questionsC, dupesC, answersC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "questionsC, dupesC, answersC = select_data(questions, dupes, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training and scoring datasets\n",
    "\n",
    "We split questions based on the creation date so that the training and the test sets are defined as below.\n",
    "1. training set = Original quesiton + 75% of oldest Duplications per Original question\n",
    "2. test set = remaining 25% of Duplications per Original question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a function to split Original questions and their Duplications into training and test sets.\n",
    "def split_data(questions, dupes, frac):\n",
    "    trainQ = questions\n",
    "    testQ = pd.DataFrame(columns = dupes.columns.values) # create an empty data frame\n",
    "    for answerId in np.unique(dupes.AnswerId):\n",
    "        df = dupes.query('AnswerId == ' + str(answerId))\n",
    "        totalCount = len(df)\n",
    "        splitPoint = int(totalCount * frac)\n",
    "        dfSort = df.sort_values(by = ['CreationDate'])\n",
    "        trainQ = trainQ.append(dfSort.head(splitPoint)) # oldest N percent of duplications\n",
    "        testQ = testQ.append(dfSort.tail(totalCount - splitPoint))\n",
    "    return trainQ, testQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare training and test\n",
    "trainQ, testQ = split_data(questionsC, dupesC, 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process text data\n",
    "\n",
    "The CleanAndSplitText function below takes as input a list where each row element is a single cohesive long string of text, i.e. a \"document\". The function first splits each string by various forms of punctuation into chunks of text that are likely sentences, phrases or sub-phrases. The splitting is designed to prohibit the phrase learning process from using cross-sentence or cross-phrase word strings when learning phrases.\n",
    "\n",
    "The function creates a table where each row represents a chunk of text from the original documents. The DocIndex coulmn indicates the original row index from associated document in the input from which the chunk of text originated. The TextLine column contains the original text excluding the punctuation marks and HTML markup that have been during the cleaning process.The TextLineLower column contains a fully lower-cased verion of the text in the TextLIne column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CleanAndSplitText(frame):\n",
    "\n",
    "    textDataOut = [] \n",
    "\n",
    "    # This regular expression is for punctuation that we wish to clean out\n",
    "    # We also will split sentences into smaller phrase like units using this expression\n",
    "    rePhraseBreaks = re.compile(\"[\\\"\\!\\?\\)\\]\\}\\,\\:\\;\\*\\-]*\\s+\\([0-9]+\\)\\s+[\\(\\[\\{\\\"\\*\\-]*\"   #                           \n",
    "                                \"|[\\\"\\!\\?\\)\\]\\}\\,\\:\\;\\*\\-]+\\s+[\\(\\[\\{\\\"\\*\\-]*\"\n",
    "                                \"|\\.\\.+\"       # ..\n",
    "                                \"|\\s*\\-\\-+\\s*\" # --\n",
    "                                \"|\\s+\\-\\s+\"    # -  \n",
    "                                \"|\\:\\:+\"       # ::\n",
    "                                \"|\\s+[\\/\\(\\[\\{\\\"\\-\\*]+\\s*\"  \n",
    "                                \"|[\\,!\\?\\\"\\)\\(\\]\\[\\}\\{\\:\\;\\*](?=[a-zA-Z])\"\n",
    "                                \"|[\\\"\\!\\?\\)\\]\\}\\,\\:\\;]+[\\.]*$\"\n",
    "                             )\n",
    "    \n",
    "    # Regex for underbars\n",
    "    regexUnderbar = re.compile('_|_+')\n",
    "    \n",
    "    # Regex for space\n",
    "    regexSpace = re.compile(' +')\n",
    " \n",
    "    # Regex for sentence final period\n",
    "    regexPeriod = re.compile(\"\\.$\")\n",
    "    \n",
    "    # Regex for parentheses\n",
    "    regexParentheses = re.compile(\"\\(\\$?\")\n",
    "    \n",
    "    # Regex for equal sign\n",
    "    regexEqual = re.compile(\"=\")\n",
    "\n",
    "    # Iterate through each document and do:\n",
    "    #    (1) Split documents into sections based on section headers and remove section headers\n",
    "    #    (2) Split the sections into sentences using NLTK sentence tokenizer\n",
    "    #    (3) Further split sentences into phrasal units based on punctuation and remove punctuation\n",
    "    #    (4) Remove sentence final periods when not part of a abbreviation \n",
    "\n",
    "    for i in range(0,len(frame)):\n",
    "        \n",
    "        # Extract one document from frame\n",
    "        docID = frame.index.values[i]\n",
    "        docText = frame['Text'].iloc[i] \n",
    "\n",
    "        # Set counter for output line count for this document\n",
    "        lineIndex=0\n",
    "\n",
    "        sentences = SENTENCE_BREAKER.tokenize(docText)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "\n",
    "            # Split each sentence into phrase level chunks based on punctuation\n",
    "            textSegs = rePhraseBreaks.split(sentence)\n",
    "            numSegs = len(textSegs)\n",
    "\n",
    "            for j in range(0,numSegs):\n",
    "                if len(textSegs[j])>0:\n",
    "                    # Convert underbars to spaces \n",
    "                    # Underbars are reserved for building the compound word phrases                   \n",
    "                    textSegs[j] = regexUnderbar.sub(\" \",textSegs[j])\n",
    "                    \n",
    "                    # Split out the words so we can specially handle the last word\n",
    "                    words = regexSpace.split(textSegs[j])\n",
    "                    \n",
    "                    # Remove parentheses and equal signs\n",
    "                    words = [regexEqual.sub(\"\", regexParentheses.sub(\"\", w)) for w in words]\n",
    "                    \n",
    "                    phraseOut = \"\"\n",
    "                    last = len(words) -1\n",
    "                    for i in range(0, last):\n",
    "                        phraseOut += words[i] + \" \"\n",
    "                    # If the last word ends in a period then remove the period\n",
    "                    lastWord = regexPeriod.sub(\"\", words[last])\n",
    "                    # If the last word is an abbreviation like \"U.S.\"\n",
    "                    # then add the word final perios back on\n",
    "                    if \"\\.\" in lastWord:\n",
    "                        lastWord += \".\"\n",
    "                    phraseOut += lastWord    \n",
    "\n",
    "                    textDataOut.append([docID,lineIndex,phraseOut, phraseOut.lower()])\n",
    "                    lineIndex += 1\n",
    "                        \n",
    "    # Convert to pandas frame \n",
    "    frameOut = pd.DataFrame(textDataOut, columns=['DocID','DocLine','CleanedText', 'LowercaseText'])                      \n",
    "    \n",
    "    return frameOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CleanedTrainQ = CleanAndSplitText(trainQ)\n",
    "CleanedTestQ = CleanAndSplitText(testQ)\n",
    "CleanedAnswers = CleanAndSplitText(answersC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>DocLine</th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>LowercaseText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>220231</td>\n",
       "      <td>0</td>\n",
       "      <td>accessing the web page's http headers in javas...</td>\n",
       "      <td>accessing the web page's http headers in javas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>220231</td>\n",
       "      <td>1</td>\n",
       "      <td>how do i access a page's http response headers...</td>\n",
       "      <td>how do i access a page's http response headers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DocID  DocLine                                        CleanedText  \\\n",
       "0  220231        0  accessing the web page's http headers in javas...   \n",
       "1  220231        1  how do i access a page's http response headers...   \n",
       "\n",
       "                                       LowercaseText  \n",
       "0  accessing the web page's http headers in javas...  \n",
       "1  how do i access a page's http response headers...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CleanedTrainQ.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute N-gram Statistics for Phrase Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is Step 1 for each iteration of phrase learning\n",
    "# We count the number of occurances of all 2-gram, 3-ngram, and 4-gram\n",
    "# word sequences \n",
    "def ComputeNgramStats(textData,functionwordHash,blacklistHash):\n",
    "    \n",
    "    # Create an array to store the total count of all ngrams up to 4-grams\n",
    "    # Array element 0 is unused, element 1 is unigrams, element 2 is bigrams, etc.\n",
    "    ngramCounts = [0]*5;\n",
    "       \n",
    "    # Create a list of structures to tabulate ngram count statistics\n",
    "    # Array element 0 is the array of total ngram counts,\n",
    "    # Array element 1 is a hash table of individual unigram counts\n",
    "    # Array element 2 is a hash table of individual bigram counts\n",
    "    # Array element 3 is a hash table of individual trigram counts\n",
    "    # Array element 4 is a hash table of individual 4-gram counts\n",
    "    ngramStats = [ngramCounts, {}, {}, {}, {}]\n",
    "          \n",
    "    # Create a regular expression for assessing validity of words\n",
    "    # for phrase modeling. The expression says words in phrases\n",
    "    # must either:\n",
    "    # (1) contain an alphabetic character, or \n",
    "    # (2) be the single charcater '&', or\n",
    "    # (3) be a one or two digit number\n",
    "    reWordIsValid = re.compile('[A-Za-z]|^&$|^\\d\\d?$')\n",
    "    \n",
    "    # Go through the text data line by line collecting count statistics\n",
    "    # for all valid n-grams that could appear in a potential phrase\n",
    "    numLines = len(textData)\n",
    "    for i in range(0, numLines):\n",
    "\n",
    "        # Split the text line into an array of words\n",
    "        wordArray = textData[i].split()\n",
    "        numWords = len(wordArray)\n",
    "        \n",
    "        # Create an array marking each word as valid or invalid\n",
    "        validArray = [];\n",
    "        for word in wordArray:\n",
    "            validArray.append(reWordIsValid.match(word) != None)        \n",
    "            \n",
    "        # Tabulate total raw ngrams for this line into counts for each ngram bin\n",
    "        # The total ngrams counts include the counts of all ngrams including those\n",
    "        # that we won't consider as parts of phrases\n",
    "        for j in range(1,5):\n",
    "            if j<=numWords:\n",
    "                ngramCounts[j] += numWords - j + 1 \n",
    "        \n",
    "        # Collect counts for viable phrase ngrams and left context sub-phrases\n",
    "        for j in range(0,numWords):\n",
    "            word = wordArray[j]\n",
    "\n",
    "            # Only bother counting the ngrams that start with a valid content word\n",
    "            # i.e., valids words not in the function word list or the black list\n",
    "            if ( ( word not in functionwordHash ) and ( word not in blacklistHash ) and validArray[j] ):\n",
    "\n",
    "                # Initialize ngram string with first content word and add it to unigram counts\n",
    "                ngramSeq = word \n",
    "                if ngramSeq in ngramStats[1]:\n",
    "                    ngramStats[1][ngramSeq] += 1\n",
    "                else:\n",
    "                    ngramStats[1][ngramSeq] = 1\n",
    "\n",
    "                # Count valid ngrams from bigrams up to 4-grams\n",
    "                stop = 0\n",
    "                k = 1\n",
    "                while (k<4) and (j+k<numWords) and not stop:\n",
    "                    n = k + 1\n",
    "                    nextNgramWord = wordArray[j+k]\n",
    "                    # Only count ngrams with valid words not in the blacklist\n",
    "                    if ( validArray[j+k] and nextNgramWord not in blacklistHash ):\n",
    "                        ngramSeq += \" \" + nextNgramWord\n",
    "                        if ngramSeq in ngramStats[n]:\n",
    "                            ngramStats[n][ngramSeq] += 1\n",
    "                        else:\n",
    "                            ngramStats[n][ngramSeq] = 1 \n",
    "                        k += 1\n",
    "                        if nextNgramWord not in functionwordHash:\n",
    "                            # Stop counting new ngrams after second content word in \n",
    "                            # ngram is reached and ngram is a viable full phrase\n",
    "                            stop = 1\n",
    "                    else:\n",
    "                        stop = 1\n",
    "    return ngramStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank Potential Phrases by the Weighted Pointwise Mutual Information of their Constituent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RankNgrams(ngramStats,functionwordHash,minCount):\n",
    "    # Create a hash table to store weighted pointwise mutual \n",
    "    # information scores for each viable phrase\n",
    "    ngramWPMIHash = {}\n",
    "        \n",
    "    # Go through each of the ngram tables and compute the phrase scores\n",
    "    # for the viable phrases\n",
    "    for n in range(2,5):\n",
    "        i = n-1\n",
    "        for ngram in ngramStats[n].keys():\n",
    "            ngramCount = ngramStats[n][ngram]\n",
    "            if ngramCount >= minCount:\n",
    "                wordArray = ngram.split()\n",
    "                # If the final word in the ngram is not a function word then\n",
    "                # the ngram is a valid phrase candidate we want to score\n",
    "                if wordArray[i] not in functionwordHash: \n",
    "                    leftNgram = wordArray[0]\n",
    "                    for j in range(1,i):\n",
    "                        leftNgram += ' ' + wordArray[j]\n",
    "                    rightWord = wordArray[i]\n",
    "                    \n",
    "                    # Compute the weighted pointwise mutual information (WPMI) for the phrase\n",
    "                    probNgram = float(ngramStats[n][ngram])/float(ngramStats[0][n])\n",
    "                    probLeftNgram = float(ngramStats[n-1][leftNgram])/float(ngramStats[0][n-1])\n",
    "                    probRightWord = float(ngramStats[1][rightWord])/float(ngramStats[0][1])\n",
    "                    WPMI = probNgram * math.log(probNgram/(probLeftNgram*probRightWord));\n",
    "\n",
    "                    # Add the phrase into the list of scored phrases only if WMPI is positive\n",
    "                    if WPMI > 0:\n",
    "                        ngramWPMIHash[ngram] = WPMI  \n",
    "    \n",
    "    # Create a sorted list of the phrase candidates\n",
    "    rankedNgrams = sorted(ngramWPMIHash, key=ngramWPMIHash.__getitem__, reverse=True)\n",
    "\n",
    "    # Force a memory clean-up\n",
    "    ngramWPMIHash = None\n",
    "    gc.collect()\n",
    "\n",
    "    return rankedNgrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Phrase Rewrites to Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ApplyPhraseRewrites(rankedNgrams,textData,learnedPhrases,                 \n",
    "                        maxPhrasesToAdd,maxPhraseLength,verbose):\n",
    "\n",
    "    # This function will consider at most maxRewrite \n",
    "    # new phrases to be added into the learned phrase \n",
    "    # list as specified by the calling fuinction\n",
    "    maxRewrite=maxPhrasesToAdd\n",
    "\n",
    "    # If the remaining number of proposed ngram phrases is less \n",
    "    # than the max allowed, then reset maxRewrite to the size of \n",
    "    # the proposed ngram phrases list\n",
    "    numNgrams = len(rankedNgrams)\n",
    "    if numNgrams < maxRewrite:\n",
    "        maxRewrite = numNgrams\n",
    "    \n",
    "    # Create empty hash tables to keep track of phrase overlap conflicts\n",
    "    leftConflictHash = {}\n",
    "    rightConflictHash = {}\n",
    "    \n",
    "    # Create an empty hash table collecting the set of rewrite rules\n",
    "    # to be applied during this iteration of phrase learning\n",
    "    ngramRewriteHash = {}\n",
    "    \n",
    "    # Precompile the regex for finding spaces in ngram phrases\n",
    "    regexSpace = re.compile(' ')\n",
    "\n",
    "    # Initialize some bookkeeping variables\n",
    "    numLines = len(textData)\n",
    "    numPhrasesAdded = 0\n",
    "    numConsidered = 0\n",
    "    lastSkippedNgram = \"\"\n",
    "    lastAddedNgram = \"\"\n",
    "  \n",
    "    # Collect list up to maxRewrite ngram phrase rewrites\n",
    "    stop = False\n",
    "    index = 0\n",
    "    while not stop:\n",
    "\n",
    "        # Get the next phrase to consider adding to the phrase list\n",
    "        inputNgram = rankedNgrams[index]\n",
    "\n",
    "        # Create the output compound word version of the phrase\n",
    "        # The extra space is added to make the regex rewrite easier\n",
    "        outputNgram = \" \" + regexSpace.sub(\"_\",inputNgram)\n",
    "\n",
    "        # Count the total number of words in the proposed phrase\n",
    "        numWords = len(outputNgram.split(\"_\"))\n",
    "\n",
    "        # Only add phrases that don't exceed the max phrase length\n",
    "        if (numWords <= maxPhraseLength):\n",
    "    \n",
    "            # Keep count of phrases considered for inclusion during this iteration\n",
    "            numConsidered += 1\n",
    "\n",
    "            # Extract the left and right words in the phrase to use\n",
    "            # in checks for phrase overlap conflicts\n",
    "            ngramArray = inputNgram.split()\n",
    "            leftWord = ngramArray[0]\n",
    "            rightWord = ngramArray[len(ngramArray)-1]\n",
    "\n",
    "            # Skip any ngram phrases that conflict with earlier phrases added\n",
    "            # These ngram phrases will be reconsidered in the next iteration\n",
    "            if (leftWord in leftConflictHash) or (rightWord in rightConflictHash): \n",
    "                if verbose: \n",
    "                    print (\"(%d) Skipping (context conflict): %s\" % (numConsidered,inputNgram))\n",
    "                lastSkippedNgram = inputNgram\n",
    "                \n",
    "            # If no conflict exists then add this phrase into the list of phrase rewrites     \n",
    "            else: \n",
    "                if verbose:\n",
    "                    print (\"(%d) Adding: %s\" % (numConsidered,inputNgram))\n",
    "                ngramRewriteHash[\" \" + inputNgram] = outputNgram\n",
    "                learnedPhrases.append(inputNgram) \n",
    "                lastAddedNgram = inputNgram\n",
    "                numPhrasesAdded += 1\n",
    "            \n",
    "            # Keep track of all context words that might conflict with upcoming\n",
    "            # propose phrases (even when phrases are skipped instead of added)\n",
    "            leftConflictHash[rightWord] = 1\n",
    "            rightConflictHash[leftWord] = 1\n",
    "\n",
    "            # Stop when we've considered the maximum number of phrases per iteration\n",
    "            if ( numConsidered >= maxRewrite ):\n",
    "                stop = True\n",
    "            \n",
    "        # Increment to next phrase\n",
    "        index += 1\n",
    "    \n",
    "        # Stop if we've reached the end of the ranked ngram list\n",
    "        if index >= len(rankedNgrams):\n",
    "            stop = True\n",
    "\n",
    "    # Now do the phrase rewrites over the entire set of text data\n",
    "    if numPhrasesAdded == 1:\n",
    "        # If only one phrase to add use a single regex rule to do this phrase rewrite        \n",
    "        inputNgram = \" \" + lastAddedNgram\n",
    "        outputNgram = ngramRewriteHash[inputNgram]\n",
    "        regexNgram = re.compile (r'%s(?= )' % re.escape(inputNgram)) \n",
    "        # Apply the regex over the full data set\n",
    "        for j in range(0,numLines):\n",
    "            textData[j] = regexNgram.sub(outputNgram, textData[j])\n",
    "    elif numPhrasesAdded > 1:\n",
    "        # Compile a single regex rule from the collected set of phrase rewrites for this iteration\n",
    "        ngramRegex = re.compile(r'%s(?= )' % \"|\".join(map(re.escape, ngramRewriteHash.keys())))\n",
    "        # Apply the regex over the full data set\n",
    "        for i in range(0,len(textData)):\n",
    "            # The regex substituion looks up the output string rewrite  \n",
    "            # in the hash table for each matched input phrase regex\n",
    "            textData[i] = ngramRegex.sub(lambda mo: ngramRewriteHash[mo.string[mo.start():mo.end()]], textData[i]) \n",
    "      \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the full iterative phrase learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ApplyPhraseLearning(textData,learnedPhrases,learningSettings):\n",
    "    \n",
    "    stop = 0\n",
    "    iterNum = 0\n",
    "\n",
    "    # Get the learning parameters from the structue passed in by thee calling function\n",
    "    maxNumPhrases = learningSettings.maxNumPhrases\n",
    "    maxPhraseLength = learningSettings.maxPhraseLength\n",
    "    functionwordHash = learningSettings.functionwordHash\n",
    "    blacklistHash = learningSettings.blacklistHash\n",
    "    verbose = learningSettings.verbose\n",
    "    minCount = learningSettings.minInstanceCount\n",
    "    \n",
    "    # Start timing the process\n",
    "    functionStartTime = time.clock()\n",
    "    \n",
    "    numPhrasesLearned = len(learnedPhrases)\n",
    "    print (\"Start phrase learning with %d phrases of %d phrases learned\" % (numPhrasesLearned,maxNumPhrases))\n",
    "\n",
    "    while not stop:\n",
    "        iterNum += 1\n",
    "                \n",
    "        # Start timing this iteration\n",
    "        startTime = time.clock()\n",
    " \n",
    "        # Collect ngram stats\n",
    "        ngramStats = ComputeNgramStats(textData,functionwordHash,blacklistHash)\n",
    "\n",
    "        # Rank ngrams\n",
    "        rankedNgrams = RankNgrams(ngramStats,functionwordHash,minCount)\n",
    "        \n",
    "        # Incorporate top ranked phrases into phrase list\n",
    "        # and rewrite the text to use these phrases\n",
    "        maxPhrasesToAdd = maxNumPhrases - numPhrasesLearned\n",
    "        if maxPhrasesToAdd > learningSettings.maxPhrasesPerIter:\n",
    "            maxPhrasesToAdd = learningSettings.maxPhrasesPerIter\n",
    "        ApplyPhraseRewrites(rankedNgrams,textData,learnedPhrases,maxPhrasesToAdd,maxPhraseLength,verbose)\n",
    "        numPhrasesAdded = len(learnedPhrases) - numPhrasesLearned\n",
    "\n",
    "        # Garbage collect\n",
    "        ngramStats = None\n",
    "        rankedNgrams = None\n",
    "        gc.collect();\n",
    "               \n",
    "        elapsedTime = time.clock() - startTime\n",
    "\n",
    "        numPhrasesLearned = len(learnedPhrases)\n",
    "        print (\"Iteration %d: Added %d new phrases in %.2f seconds (Learned %d of max %d)\" % \n",
    "               (iterNum,numPhrasesAdded,elapsedTime,numPhrasesLearned,maxNumPhrases))\n",
    "        \n",
    "        if numPhrasesAdded >= maxPhrasesToAdd or numPhrasesAdded == 0:\n",
    "            stop = 1\n",
    "        \n",
    "    # Remove the space padding at the start and end of each line\n",
    "    regexSpacePadding = re.compile('^ +| +$')\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i] = regexSpacePadding.sub(\"\",textData[i])\n",
    "    \n",
    "    gc.collect()\n",
    " \n",
    "    elapsedTime = time.clock() - functionStartTime\n",
    "    elapsedTimeHours = elapsedTime/3600.0;\n",
    "    print (\"*** Phrase learning completed in %.2f hours ***\" % elapsedTimeHours) \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a structure defining the settings and word lists used during the phrase learning\n",
    "learningSettings = namedtuple('learningSettings',['maxNumPhrases','maxPhrasesPerIter',\n",
    "                                                  'maxPhraseLength','minInstanceCount'\n",
    "                                                  'functionwordHash','blacklistHash','verbose'])\n",
    "\n",
    "# If true it prints out the learned phrases to stdout buffer\n",
    "# while its learning. This will generate a lot of text to stdout, \n",
    "# so best to turn this off except for testing and debugging\n",
    "learningSettings.verbose = False\n",
    "\n",
    "# Maximium number of phrases to learn\n",
    "# If you want to test the code out quickly then set this to a small\n",
    "# value (e.g. 100) and set verbose to true when running the quick test\n",
    "learningSettings.maxNumPhrases = 200\n",
    "\n",
    "# Maximum number of phrases to learn per iteration \n",
    "# Increasing this number may speed up processing but will affect the ordering of the phrases \n",
    "# learned and good phrases could be by-passed if the maxNumPhrases is set to a small number\n",
    "learningSettings.maxPhrasesPerIter = 50\n",
    "\n",
    "# Maximum number of words allowed in the learned phrases \n",
    "learningSettings.maxPhraseLength = 7\n",
    "\n",
    "# Minimum number of times a phrase must occur in the data to \n",
    "# be considered during the phrase learning process\n",
    "learningSettings.minInstanceCount = 5\n",
    "\n",
    "# This is a precreated hash table containing the list \n",
    "# of function words used during phrase learning\n",
    "learningSettings.functionwordHash = functionwordHash\n",
    "\n",
    "# This is a precreated hash table containing the list \n",
    "# of black list words to be ignored during phrase learning\n",
    "learningSettings.blacklistHash = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start phrase learning with 0 phrases of 200 phrases learned\n",
      "Iteration 1: Added 41 new phrases in 2.02 seconds (Learned 41 of max 200)\n",
      "Iteration 2: Added 39 new phrases in 1.93 seconds (Learned 80 of max 200)\n",
      "Iteration 3: Added 43 new phrases in 2.00 seconds (Learned 123 of max 200)\n",
      "Iteration 4: Added 44 new phrases in 1.98 seconds (Learned 167 of max 200)\n",
      "Iteration 5: Added 27 new phrases in 1.89 seconds (Learned 194 of max 200)\n",
      "Iteration 6: Added 5 new phrases in 1.97 seconds (Learned 199 of max 200)\n",
      "Iteration 7: Added 1 new phrases in 1.70 seconds (Learned 200 of max 200)\n",
      "*** Phrase learning completed in 0.00 hours ***\n"
     ]
    }
   ],
   "source": [
    "###### Questions:\n",
    "# Initialize an empty list of learned phrases\n",
    "# If you have completed a partial run of phrase learning\n",
    "# and want to add more phrases, you can use the pre-learned \n",
    "# phrases as a starting point instead and the new phrases\n",
    "# will be appended to the list\n",
    "learnedPhrasesQ = []\n",
    "\n",
    "# Create a copy of the original text data that will be used during learning\n",
    "# The copy is needed because the algorithm does in-place replacement of learned\n",
    "# phrases directly on the text data structure it is provided\n",
    "phraseTextDataQ = []\n",
    "for textLine in CleanedTrainQ['LowercaseText']:\n",
    "    phraseTextDataQ.append(' ' + textLine + ' ')\n",
    "\n",
    "# Run the phrase learning algorithm\n",
    "ApplyPhraseLearning(phraseTextDataQ,learnedPhrasesQ,learningSettings)\n",
    "\n",
    "# Add text with learned phrases back into data frame\n",
    "CleanedTrainQ['TextWithPhrases'] = phraseTextDataQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start phrase learning with 0 phrases of 200 phrases learned\n",
      "Iteration 1: Added 44 new phrases in 0.45 seconds (Learned 44 of max 200)\n",
      "Iteration 2: Added 42 new phrases in 0.48 seconds (Learned 86 of max 200)\n",
      "Iteration 3: Added 39 new phrases in 0.50 seconds (Learned 125 of max 200)\n",
      "Iteration 4: Added 34 new phrases in 0.40 seconds (Learned 159 of max 200)\n",
      "Iteration 5: Added 34 new phrases in 0.39 seconds (Learned 193 of max 200)\n",
      "Iteration 6: Added 6 new phrases in 0.39 seconds (Learned 199 of max 200)\n",
      "Iteration 7: Added 1 new phrases in 0.39 seconds (Learned 200 of max 200)\n",
      "*** Phrase learning completed in 0.00 hours ***\n"
     ]
    }
   ],
   "source": [
    "###### Answers:\n",
    "# Initialize an empty list of learned phrases\n",
    "# If you have completed a partial run of phrase learning\n",
    "# and want to add more phrases, you can use the pre-learned \n",
    "# phrases as a starting point instead and the new phrases\n",
    "# will be appended to the list\n",
    "learnedPhrasesA = []\n",
    "\n",
    "# Create a copy of the original text data that will be used during learning\n",
    "# The copy is needed because the algorithm does in-place replacement of learned\n",
    "# phrases directly on the text data structure it is provided\n",
    "phraseTextDataA = []\n",
    "for textLine in CleanedAnswers['LowercaseText']:\n",
    "    phraseTextDataA.append(' ' + textLine + ' ')\n",
    "\n",
    "# Run the phrase learning algorithm\n",
    "ApplyPhraseLearning(phraseTextDataA,learnedPhrasesA,learningSettings)\n",
    "\n",
    "# Add text with learned phrases back into data frame\n",
    "CleanedAnswers['TextWithPhrases'] = phraseTextDataA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Learned Phrases to Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ApplyPhraseRewritesInPlace(textFrame, textColumnName, phraseRules):\n",
    "        \n",
    "    # Get text data column from frame\n",
    "    textData = textFrame[textColumnName]\n",
    "    numLines = len(textData)\n",
    "    \n",
    "    # initial a list to store output text\n",
    "    textOutput = [None] * numLines\n",
    "    \n",
    "    # Add leading and trailing spaces to make regex matching easier\n",
    "    for i in range(0,numLines):\n",
    "        textOutput[i] = \" \" + textData[i] + \" \"  \n",
    "\n",
    "    # Make sure we have phrase to add\n",
    "    numPhraseRules = len(phraseRules)\n",
    "    if numPhraseRules == 0: \n",
    "        print (\"Warning: phrase rule lise is empty - no phrases being applied to text data\")\n",
    "        return\n",
    "\n",
    "    # Precompile the regex for finding spaces in ngram phrases\n",
    "    regexSpace = re.compile(' ')\n",
    "   \n",
    "    # Initialize some bookkeeping variables\n",
    "\n",
    "    # Iterate through full set of phrases to find sets of \n",
    "    # non-conflicting phrases that can be apply simultaneously\n",
    "    index = 0\n",
    "    outerStop = False\n",
    "    while not outerStop:\n",
    "       \n",
    "        # Create empty hash tables to keep track of phrase overlap conflicts\n",
    "        leftConflictHash = {}\n",
    "        rightConflictHash = {}\n",
    "        prevConflictHash = {}\n",
    "    \n",
    "        # Create an empty hash table collecting the next set of rewrite rules\n",
    "        # to be applied during this iteration of phrase rewriting\n",
    "        phraseRewriteHash = {}\n",
    "    \n",
    "        # Progress through phrases until the next conflicting phrase is found\n",
    "        innerStop = 0\n",
    "        numPhrasesAdded = 0\n",
    "        while not innerStop:\n",
    "        \n",
    "            # Get the next phrase to consider adding to the phrase list\n",
    "            nextPhrase = phraseRules[index]            \n",
    "            \n",
    "            # Extract the left and right sides of the phrase to use\n",
    "            # in checks for phrase overlap conflicts\n",
    "            ngramArray = nextPhrase.split()\n",
    "            leftWord = ngramArray[0]\n",
    "            rightWord = ngramArray[len(ngramArray)-1] \n",
    "\n",
    "            # Stop if we reach any phrases that conflicts with earlier phrases in this iteration\n",
    "            # These ngram phrases will be reconsidered in the next iteration\n",
    "            if ((leftWord in leftConflictHash) or (rightWord in rightConflictHash) \n",
    "                or (leftWord in prevConflictHash) or (rightWord in prevConflictHash)): \n",
    "                innerStop = True\n",
    "                \n",
    "            # If no conflict exists then add this phrase into the list of phrase rewrites     \n",
    "            else: \n",
    "                # Create the output compound word version of the phrase\n",
    "                                \n",
    "                outputPhrase = regexSpace.sub(\"_\",nextPhrase);\n",
    "                \n",
    "                # Keep track of all context words that might conflict with upcoming\n",
    "                # propose phrases (even when phrases are skipped instead of added)\n",
    "                leftConflictHash[rightWord] = 1\n",
    "                rightConflictHash[leftWord] = 1\n",
    "                prevConflictHash[outputPhrase] = 1           \n",
    "                \n",
    "                # Add extra space to input an output versions of the current phrase \n",
    "                # to make the regex rewrite easier\n",
    "                outputPhrase = \" \" + outputPhrase\n",
    "                lastAddedPhrase = \" \" + nextPhrase\n",
    "                \n",
    "                # Add the phrase to the rewrite hash\n",
    "                phraseRewriteHash[lastAddedPhrase] = outputPhrase\n",
    "                  \n",
    "                # Increment to next phrase\n",
    "                index += 1\n",
    "                numPhrasesAdded  += 1\n",
    "    \n",
    "                # Stop if we've reached the end of the phrases list\n",
    "                if index >= numPhraseRules:\n",
    "                    innerStop = True\n",
    "                    outerStop = True\n",
    "                    \n",
    "        # Now do the phrase rewrites over the entire set of text data\n",
    "        if numPhrasesAdded == 1:\n",
    "        \n",
    "            # If only one phrase to add use a single regex rule to do this phrase rewrite        \n",
    "            outputPhrase = phraseRewriteHash[lastAddedPhrase]\n",
    "            regexPhrase = re.compile (r'%s(?= )' % re.escape(lastAddedPhrase)) \n",
    "        \n",
    "            # Apply the regex over the full data set\n",
    "            for j in range(0,numLines):\n",
    "                textOutput[j] = regexPhrase.sub(outputPhrase, textOutput[j])\n",
    "       \n",
    "        elif numPhrasesAdded > 1:\n",
    "            # Compile a single regex rule from the collected set of phrase rewrites for this iteration\n",
    "            regexPhrase = re.compile(r'%s(?= )' % \"|\".join(map(re.escape, phraseRewriteHash.keys())))\n",
    "            \n",
    "            # Apply the regex over the full data set\n",
    "            for i in range(0,numLines):\n",
    "                # The regex substituion looks up the output string rewrite  \n",
    "                # in the hash table for each matched input phrase regex\n",
    "                textOutput[i] = regexPhrase.sub(lambda mo: phraseRewriteHash[mo.string[mo.start():mo.end()]], textOutput[i]) \n",
    "    \n",
    "    # Remove the space padding at the start and end of each line\n",
    "    regexSpacePadding = re.compile('^ +| +$')\n",
    "    for i in range(0,len(textOutput)):\n",
    "        textOutput[i] = regexSpacePadding.sub(\"\",textOutput[i])\n",
    "    \n",
    "    return textOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CleanedTestQ['TextWithPhrases'] = ApplyPhraseRewritesInPlace(CleanedTestQ, 'LowercaseText', learnedPhrasesQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocID                                  15762825\n",
       "DocLine                                       9\n",
       "CleanedText        javascript html code c# code\n",
       "LowercaseText      javascript html code c# code\n",
       "TextWithPhrases    javascript html_code c# code\n",
       "Name: 9, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CleanedTestQ.loc[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct the Full Processed Text of Each Document and Put it into a New Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReconstituteDocsFromChunks(textData, idColumnName, textColumnName):\n",
    "    dataOut = []\n",
    "    \n",
    "    currentDoc = \"\";\n",
    "    currentDocID = \"\";\n",
    "    \n",
    "    for i in range(0,len(textData)):\n",
    "        textChunk = textData[textColumnName][i]\n",
    "        docID = textData[idColumnName][i]\n",
    "        if docID != currentDocID:\n",
    "            if currentDocID != \"\":\n",
    "                dataOut.append(currentDoc)\n",
    "            currentDoc = textChunk\n",
    "            currentDocID = docID\n",
    "        else:\n",
    "            currentDoc += \" \" + textChunk\n",
    "    dataOut.append(currentDoc)\n",
    "    \n",
    "    return dataOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainQ['TextWithPhrases'] = ReconstituteDocsFromChunks(CleanedTrainQ, 'DocID', 'TextWithPhrases')\n",
    "testQ['TextWithPhrases'] = ReconstituteDocsFromChunks(CleanedTestQ, 'DocID', 'TextWithPhrases')\n",
    "answersC['TextWithPhrases'] = ReconstituteDocsFromChunks(CleanedAnswers, 'DocID', 'TextWithPhrases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Vocabulary With Filtering Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CreateVocabForTopicModeling(textData,stopwordHash):\n",
    "\n",
    "    print (\"Counting words\")\n",
    "    numDocs = len(textData) \n",
    "    globalWordCountHash = {} \n",
    "    globalDocCountHash = {} \n",
    "    for textLine in textData:\n",
    "        docWordCountHash = {}\n",
    "        for word in textLine.split():\n",
    "            if word in globalWordCountHash:\n",
    "                globalWordCountHash[word] += 1\n",
    "            else:\n",
    "                globalWordCountHash[word] = 1\n",
    "            if word not in docWordCountHash: \n",
    "                docWordCountHash[word] = 1\n",
    "                if word in globalDocCountHash:\n",
    "                    globalDocCountHash[word] += 1\n",
    "                else:\n",
    "                    globalDocCountHash[word] = 1\n",
    "\n",
    "    minWordCount = 5;\n",
    "    minDocCount = 2;\n",
    "    maxDocFreq = .25;\n",
    "    vocabCount = 0;\n",
    "    vocabHash = {}\n",
    "\n",
    "    excStopword = 0\n",
    "    excNonalphabetic = 0\n",
    "    excMinwordcount = 0\n",
    "    excNotindochash = 0\n",
    "    excMindoccount = 0\n",
    "    excMaxdocfreq =0\n",
    "\n",
    "    print (\"Building vocab\")\n",
    "    for word in globalWordCountHash.keys():\n",
    "        # Test vocabulary exclusion criteria for each word\n",
    "        if ( word in stopwordHash ):\n",
    "            excStopword += 1\n",
    "        elif ( not re.search(r'[a-zA-Z]', word, 0) ):\n",
    "            excNonalphabetic += 1\n",
    "        elif ( globalWordCountHash[word] < minWordCount ):\n",
    "            excMinwordcount += 1\n",
    "        elif ( word not in globalDocCountHash ):\n",
    "            print (\"Warning: Word '%s' not in doc count hash\") % (word)\n",
    "            excNotindochash += 1\n",
    "        elif ( globalDocCountHash[word] < minDocCount ):\n",
    "            excMindoccount += 1\n",
    "        elif ( float(globalDocCountHash[word])/float(numDocs) > maxDocFreq ):\n",
    "            excMaxdocfreq += 1\n",
    "        else:\n",
    "            # Add word to vocab\n",
    "            vocabHash[word]= globalWordCountHash[word];\n",
    "            vocabCount += 1 \n",
    "    print (\"Excluded %d stop words\" % (excStopword))       \n",
    "    print (\"Excluded %d non-alphabetic words\" % (excNonalphabetic))  \n",
    "    print (\"Excluded %d words below word count threshold\" % (excMinwordcount)) \n",
    "    print (\"Excluded %d words below doc count threshold\" % (excMindoccount))\n",
    "    print (\"Excluded %d words above max doc frequency\" % (excMaxdocfreq)) \n",
    "    print (\"Final Vocab Size: %d words\" % vocabCount)\n",
    "            \n",
    "    return vocabHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words\n",
      "Building vocab\n",
      "Excluded 310 stop words\n",
      "Excluded 1664 non-alphabetic words\n",
      "Excluded 23968 words below word count threshold\n",
      "Excluded 215 words below doc count threshold\n",
      "Excluded 3 words above max doc frequency\n",
      "Final Vocab Size: 4977 words\n",
      "Counting words\n",
      "Building vocab\n",
      "Excluded 302 stop words\n",
      "Excluded 524 non-alphabetic words\n",
      "Excluded 7701 words below word count threshold\n",
      "Excluded 56 words below doc count threshold\n",
      "Excluded 2 words above max doc frequency\n",
      "Final Vocab Size: 2258 words\n"
     ]
    }
   ],
   "source": [
    "vocabHashQ = CreateVocabForTopicModeling(trainQ['TextWithPhrases'],functionwordHash)\n",
    "vocabHashA = CreateVocabForTopicModeling(answersC['TextWithPhrases'],functionwordHash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Text with Learned Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start by tokenizing the full text string string for each document into list of tokens.\n",
    "# any token that is in not in the pre-defined set of acceptable vocabulary words is execluded.\n",
    "def TokenizeText(textData,vocabHash):\n",
    "    tokenizedText = ''\n",
    "    for token in textData.split():\n",
    "        if token in vocabHash:\n",
    "            tokenizedText += (token.strip() + ',')\n",
    "    return tokenizedText.strip(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainQ['Tokens'] = trainQ['TextWithPhrases'].apply(lambda x: TokenizeText(x, vocabHashQ))\n",
    "testQ['Tokens'] = testQ['TextWithPhrases'].apply(lambda x: TokenizeText(x, vocabHashQ))\n",
    "answersC['Tokens'] = answersC['TextWithPhrases'].apply(lambda x: TokenizeText(x, vocabHashA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"accessing,http,headers,access,page's,http,response,headers,related,question,modified,ask,accessing,two,specific,http,headers,related,access,http_request,header,fields\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example of tokenized text in training set.\n",
    "trainQ['Tokens'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cleaned data as .tsv and upload to Azure Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_upload_data(data, file_path, container_name, blob_name):\n",
    "    data.to_csv(file_path, sep='\\t', header=True, index=True, index_label='Id')\n",
    "    blob_service.put_block_blob_from_path(container_name=container_name, blob_name=blob_name, file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modify the path in below script to upload the datasets to your own Blob Storage.\n",
    "if False: \n",
    "    save_upload_data(trainQ, 'C:\\\\Users\\\\mez\\\\Desktop\\\\trainQwithTokens.tsv', 'stackoverflow', 'trainQwithTokens.tsv')\n",
    "    save_upload_data(testQ, 'C:\\\\Users\\\\mez\\\\Desktop\\\\testQwithTokens.tsv', 'stackoverflow', 'testQwithTokens.tsv')\n",
    "    save_upload_data(answersC, 'C:\\\\Users\\\\mez\\\\Desktop\\\\answersCwithTokens.tsv', 'stackoverflow', 'answersCwithTokens.tsv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
