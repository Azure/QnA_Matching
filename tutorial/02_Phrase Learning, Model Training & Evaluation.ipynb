{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QnA Matching Tutorial (Phrase Learning, Model Training & Evaluation)\n",
    "\n",
    "## Overview\n",
    "\n",
    "First part of this notebook shows how to pre-process the text data, learn the most salient phrases present in a large collection of documents and save cleaned text data in the Azure Blob Storage. These phrases can be treated as single compound word units in down-stream processes such as discriminative training. To learn the phrases, we have implemented the basic framework that combines key phrase learning and latent topic modeling as described in the paper entitled [\"Modeling Multiword Phrases with Constrained Phrases Tree for Improved Topic Modeling of Conversational Speech\"](http://people.csail.mit.edu/hazen/publications/Hazen-SLT-2012.pdf) which was originally presented in the 2012 IEEE Workshop on Spoken Language Technology. Although the paper examines the use of the technology for analyzing human-to-human conversations, the techniques are quite general and can be applied to a wide range of natural language data including news stories, legal documents, research publications, social media forum discussions, customer feedback forms, product reviews, and many more.\n",
    "\n",
    "The remaining part of the notebooks demonstrate various approaches for feature extraction, model training and evaluation.\n",
    "\n",
    "Note: This notebook series are built under Python 3.5 and NLTK 3.2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Python Modules\n",
    "\n",
    "In this notebook, we use several open-source Python packages that need to be installed in a local machine or an Azure Notebook Server. An upgrade is requested if a previous version of a package has been installed in the past.\n",
    "\n",
    "We make use of the NLTK sentence tokenization capability which takes a long string of text and splits it into sentence units. The tokenizer requires the installation of the 'punkt' tokenizer models. After importing nltk, the nltk.download() function can be used to download specific packages such as 'punkt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uncomment the below code to install/upgrade the requested Python packages.\n",
    "# !pip install --upgrade --no-deps smart_open azure pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, sys, os, gc, requests, time, math, nltk, glob, os.path, ftplib, json, base64, datetime, warnings\n",
    "from collections import (namedtuple, Counter)\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from azure.storage import CloudStorageAccount\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "EMPTY = ''\n",
    "SPACE = ' '\n",
    "nltk.download(\"punkt\")\n",
    "NLTK_PUNKT_EN = 'tokenizers/punkt/english.pickle'\n",
    "SENTENCE_BREAKER = nltk.data.load(NLTK_PUNKT_EN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read trainQ and testQ into DataFrames\n",
    "\n",
    "As we have prepared the _trainQ_ and _testQ_ from the previous notebook, we retrieve the datasets here for the further process.\n",
    "_trainQ_ contains 5,153 training examples and _testQ_ contains 1,735 test examples. Also, there are 103 unique answer classes in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions to load .txt file into a Python dictionary. \n",
    "def load_from_url(fileURL):\n",
    "    response = requests.get(fileURL, stream=True)\n",
    "    return response.text.split('\\n')\n",
    "\n",
    "def LoadListAsHash(fileURL):\n",
    "    listHash = {}\n",
    "    wordsList = load_from_url(fileURL)\n",
    "    # Read in lines one by one stripping away extra spaces, \n",
    "    # leading spaces, and trailing spaces and inserting each\n",
    "    # cleaned up line into a hash table\n",
    "    re1 = re.compile(' +')\n",
    "    re2 = re.compile('^ +| +$')\n",
    "    for stringIn in wordsList:\n",
    "        term = re2.sub(\"\",re1.sub(\" \",stringIn.strip('\\n')))\n",
    "        if term != '':\n",
    "            listHash[term] = 1\n",
    "    return listHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainQ_url = 'https://mezsa.blob.core.windows.net/stackoverflownew/trainQ_tutorial.tsv'\n",
    "testQ_url = 'https://mezsa.blob.core.windows.net/stackoverflownew/testQ_tutorial.tsv'\n",
    "# answersC_url = 'https://mezsa.blob.core.windows.net/stackoverflownew/answersC_tutorial.tsv'\n",
    "function_words_url = 'https://mezsa.blob.core.windows.net/stackoverflow/function_words.txt'\n",
    "\n",
    "# load datasets.\n",
    "trainQ = pd.read_csv(trainQ_url, sep='\\t', index_col='Id', encoding='latin1')\n",
    "testQ = pd.read_csv(testQ_url, sep='\\t', index_col='Id', encoding='latin1')\n",
    "# answersC = pd.read_csv(answersC_url, sep='\\t', index_col='Id', encoding='latin1')\n",
    "# Load the list of non-content bearing function words\n",
    "functionwordHash = LoadListAsHash(function_words_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Learning\n",
    "\n",
    "### Process Text Data\n",
    "\n",
    "The CleanAndSplitText function below takes as input a list where each row element is a single cohesive long string of text, i.e. a \"document\". The function first splits each string by various forms of punctuation into chunks of text that are likely sentences, phrases or sub-phrases. The splitting is designed to prohibit the phrase learning process from using cross-sentence or cross-phrase word strings when learning phrases.\n",
    "\n",
    "The function creates a table where each row represents a chunk of text from the original documents. The DocIndex coulmn indicates the original row index from associated document in the input from which the chunk of text originated. The TextLine column contains the original text excluding the punctuation marks and HTML markup that have been during the cleaning process. The TextLineLower column contains a fully lower-cased version of the text in the TextLine column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CleanAndSplitText(frame):\n",
    "\n",
    "    textDataOut = [] \n",
    "\n",
    "    # This regular expression is for punctuation that we wish to clean out\n",
    "    # We also will split sentences into smaller phrase like units using this expression\n",
    "    rePhraseBreaks = re.compile(\"[\\\"\\!\\?\\)\\]\\}\\,\\:\\;\\*\\-]*\\s+\\([0-9]+\\)\\s+[\\(\\[\\{\\\"\\*\\-]*\"                         \n",
    "                                \"|[\\\"\\!\\?\\)\\]\\}\\,\\:\\;\\*\\-]+\\s+[\\(\\[\\{\\\"\\*\\-]*\" \n",
    "                                \"|\\.\\.+\"       # ..\n",
    "                                \"|\\s*\\-\\-+\\s*\" # --\n",
    "                                \"|\\s+\\-\\s+\"    # -  \n",
    "                                \"|\\:\\:+\"       # ::\n",
    "                                \"|\\s+[\\/\\(\\[\\{\\\"\\-\\*]+\\s*\"  \n",
    "                                \"|[\\,!\\?\\\"\\)\\(\\]\\[\\}\\{\\:\\;\\*](?=[a-zA-Z])\"\n",
    "                                \"|[\\\"\\!\\?\\)\\]\\}\\,\\:\\;]+[\\.]*$\"\n",
    "                             )\n",
    "    \n",
    "    # Regex for underbars\n",
    "    regexUnderbar = re.compile('_|_+')\n",
    "    \n",
    "    # Regex for space\n",
    "    regexSpace = re.compile(' +')\n",
    " \n",
    "    # Regex for sentence final period\n",
    "    regexPeriod = re.compile(\"\\.$\")\n",
    "    \n",
    "    # Regex for parentheses\n",
    "    regexParentheses = re.compile(\"\\(\\$?\")\n",
    "    \n",
    "    # Regex for equal sign\n",
    "    regexEqual = re.compile(\"=\")\n",
    "\n",
    "    # Iterate through each document and do:\n",
    "    #    (1) Split documents into sections based on section headers and remove section headers\n",
    "    #    (2) Split the sections into sentences using NLTK sentence tokenizer\n",
    "    #    (3) Further split sentences into phrasal units based on punctuation and remove punctuation\n",
    "    #    (4) Remove sentence final periods when not part of a abbreviation \n",
    "\n",
    "    for i in range(0,len(frame)):\n",
    "        \n",
    "        # Extract one document from frame\n",
    "        docID = frame.index.values[i]\n",
    "        docText = frame['Text'].iloc[i] \n",
    "\n",
    "        # Set counter for output line count for this document\n",
    "        lineIndex=0\n",
    "\n",
    "        sentences = SENTENCE_BREAKER.tokenize(docText)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "\n",
    "            # Split each sentence into phrase level chunks based on punctuation\n",
    "            textSegs = rePhraseBreaks.split(sentence)\n",
    "            numSegs = len(textSegs)\n",
    "\n",
    "            for j in range(0,numSegs):\n",
    "                if len(textSegs[j])>0:\n",
    "                    # Convert underbars to spaces \n",
    "                    # Underbars are reserved for building the compound word phrases                   \n",
    "                    textSegs[j] = regexUnderbar.sub(\" \",textSegs[j])\n",
    "                    \n",
    "                    # Split out the words so we can specially handle the last word\n",
    "                    words = regexSpace.split(textSegs[j])\n",
    "                    \n",
    "                    # Remove parentheses and equal signs\n",
    "                    words = [regexEqual.sub(\"\", regexParentheses.sub(\"\", w)) for w in words]\n",
    "                    \n",
    "                    phraseOut = \"\"\n",
    "                    last = len(words) -1\n",
    "                    for i in range(0, last):\n",
    "                        phraseOut += words[i] + \" \"\n",
    "                    # If the last word ends in a period then remove the period\n",
    "                    lastWord = regexPeriod.sub(\"\", words[last])\n",
    "                    # If the last word is an abbreviation like \"U.S.\"\n",
    "                    # then add the word final perios back on\n",
    "                    if \"\\.\" in lastWord:\n",
    "                        lastWord += \".\"\n",
    "                    phraseOut += lastWord    \n",
    "\n",
    "                    textDataOut.append([docID,lineIndex,phraseOut, phraseOut.lower()])\n",
    "                    lineIndex += 1\n",
    "                        \n",
    "    # Convert to pandas frame \n",
    "    frameOut = pd.DataFrame(textDataOut, columns=['DocID','DocLine','CleanedText', 'LowercaseText'])                      \n",
    "    \n",
    "    return frameOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CleanedTrainQ = CleanAndSplitText(trainQ)\n",
    "CleanedTestQ = CleanAndSplitText(testQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>DocLine</th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>LowercaseText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69913</td>\n",
       "      <td>0</td>\n",
       "      <td>why don't self-closing script tags work</td>\n",
       "      <td>why don't self-closing script tags work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69913</td>\n",
       "      <td>1</td>\n",
       "      <td>what is the reason browsers do not correctly r...</td>\n",
       "      <td>what is the reason browsers do not correctly r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DocID  DocLine                                        CleanedText  \\\n",
       "0  69913        0            why don't self-closing script tags work   \n",
       "1  69913        1  what is the reason browsers do not correctly r...   \n",
       "\n",
       "                                       LowercaseText  \n",
       "0            why don't self-closing script tags work  \n",
       "1  what is the reason browsers do not correctly r...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CleanedTrainQ.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute N-gram Statistics for Phrase Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is Step 1 for each iteration of phrase learning\n",
    "# We count the number of occurances of all 2-gram, 3-ngram, and 4-gram\n",
    "# word sequences \n",
    "def ComputeNgramStats(textData,functionwordHash,blacklistHash):\n",
    "    \n",
    "    # Create an array to store the total count of all ngrams up to 4-grams\n",
    "    # Array element 0 is unused, element 1 is unigrams, element 2 is bigrams, etc.\n",
    "    ngramCounts = [0]*5;\n",
    "       \n",
    "    # Create a list of structures to tabulate ngram count statistics\n",
    "    # Array element 0 is the array of total ngram counts,\n",
    "    # Array element 1 is a hash table of individual unigram counts\n",
    "    # Array element 2 is a hash table of individual bigram counts\n",
    "    # Array element 3 is a hash table of individual trigram counts\n",
    "    # Array element 4 is a hash table of individual 4-gram counts\n",
    "    ngramStats = [ngramCounts, {}, {}, {}, {}]\n",
    "          \n",
    "    # Create a regular expression for assessing validity of words\n",
    "    # for phrase modeling. The expression says words in phrases\n",
    "    # must either:\n",
    "    # (1) contain an alphabetic character, or \n",
    "    # (2) be the single charcater '&', or\n",
    "    # (3) be a one or two digit number\n",
    "    reWordIsValid = re.compile('[A-Za-z]|^&$|^\\d\\d?$')\n",
    "    \n",
    "    # Go through the text data line by line collecting count statistics\n",
    "    # for all valid n-grams that could appear in a potential phrase\n",
    "    numLines = len(textData)\n",
    "    for i in range(0, numLines):\n",
    "\n",
    "        # Split the text line into an array of words\n",
    "        wordArray = textData[i].split()\n",
    "        numWords = len(wordArray)\n",
    "#         if numWords == 0:\n",
    "#             print(textData[i])\n",
    "        \n",
    "        # Create an array marking each word as valid or invalid\n",
    "        validArray = [];\n",
    "        for word in wordArray:\n",
    "            validArray.append(reWordIsValid.match(word) != None)        \n",
    "            \n",
    "        # Tabulate total raw ngrams for this line into counts for each ngram bin\n",
    "        # The total ngrams counts include the counts of all ngrams including those\n",
    "        # that we won't consider as parts of phrases\n",
    "        for j in range(1,5):\n",
    "            if j<=numWords:\n",
    "                ngramCounts[j] += numWords - j + 1 \n",
    "        \n",
    "        # Collect counts for viable phrase ngrams and left context sub-phrases\n",
    "        for j in range(0,numWords):\n",
    "            word = wordArray[j]\n",
    "\n",
    "            # Only bother counting the ngrams that start with a valid content word\n",
    "            # i.e., valids words not in the function word list or the black list\n",
    "            if ( ( word not in functionwordHash ) and ( word not in blacklistHash ) and validArray[j] ):\n",
    "\n",
    "                # Initialize ngram string with first content word and add it to unigram counts\n",
    "                ngramSeq = word \n",
    "                if ngramSeq in ngramStats[1]:\n",
    "                    ngramStats[1][ngramSeq] += 1\n",
    "                else:\n",
    "                    ngramStats[1][ngramSeq] = 1\n",
    "\n",
    "                # Count valid ngrams from bigrams up to 4-grams\n",
    "                stop = 0\n",
    "                k = 1\n",
    "                while (k<4) and (j+k<numWords) and not stop:\n",
    "                    n = k + 1\n",
    "                    nextNgramWord = wordArray[j+k]\n",
    "                    # Only count ngrams with valid words not in the blacklist\n",
    "                    if ( validArray[j+k] and nextNgramWord not in blacklistHash ):\n",
    "                        ngramSeq += \" \" + nextNgramWord\n",
    "                        if ngramSeq in ngramStats[n]:\n",
    "                            ngramStats[n][ngramSeq] += 1\n",
    "                        else:\n",
    "                            ngramStats[n][ngramSeq] = 1 \n",
    "                        k += 1\n",
    "                        if nextNgramWord not in functionwordHash:\n",
    "                            # Stop counting new ngrams after second content word in \n",
    "                            # ngram is reached and ngram is a viable full phrase\n",
    "                            stop = 1\n",
    "                    else:\n",
    "                        stop = 1\n",
    "    return ngramStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank Potential Phrases by the Weighted Pointwise Mutual Information of their Constituent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RankNgrams(ngramStats,functionwordHash,minCount):\n",
    "    # Create a hash table to store weighted pointwise mutual \n",
    "    # information scores for each viable phrase\n",
    "    ngramWPMIHash = {}\n",
    "        \n",
    "    # Go through each of the ngram tables and compute the phrase scores\n",
    "    # for the viable phrases\n",
    "    for n in range(2,5):\n",
    "        i = n-1\n",
    "        for ngram in ngramStats[n].keys():\n",
    "            ngramCount = ngramStats[n][ngram]\n",
    "            if ngramCount >= minCount:\n",
    "                wordArray = ngram.split()\n",
    "                # If the final word in the ngram is not a function word then\n",
    "                # the ngram is a valid phrase candidate we want to score\n",
    "                if wordArray[i] not in functionwordHash: \n",
    "                    leftNgram = wordArray[0]\n",
    "                    for j in range(1,i):\n",
    "                        leftNgram += ' ' + wordArray[j]\n",
    "                    rightWord = wordArray[i]\n",
    "                    \n",
    "                    # Compute the weighted pointwise mutual information (WPMI) for the phrase\n",
    "                    probNgram = float(ngramStats[n][ngram])/float(ngramStats[0][n])\n",
    "                    probLeftNgram = float(ngramStats[n-1][leftNgram])/float(ngramStats[0][n-1])\n",
    "                    probRightWord = float(ngramStats[1][rightWord])/float(ngramStats[0][1])\n",
    "                    WPMI = probNgram * math.log(probNgram/(probLeftNgram*probRightWord));\n",
    "\n",
    "                    # Add the phrase into the list of scored phrases only if WMPI is positive\n",
    "                    if WPMI > 0:\n",
    "                        ngramWPMIHash[ngram] = WPMI  \n",
    "    \n",
    "    # Create a sorted list of the phrase candidates\n",
    "    rankedNgrams = sorted(ngramWPMIHash, key=ngramWPMIHash.__getitem__, reverse=True)\n",
    "\n",
    "    # Force a memory clean-up\n",
    "    ngramWPMIHash = None\n",
    "    gc.collect()\n",
    "\n",
    "    return rankedNgrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Phrase Rewrites to Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ApplyPhraseRewrites(rankedNgrams,textData,learnedPhrases,                 \n",
    "                        maxPhrasesToAdd,maxPhraseLength,verbose):\n",
    "    \n",
    "    if len(rankedNgrams) == 0:\n",
    "        return\n",
    "    \n",
    "    # This function will consider at most maxRewrite \n",
    "    # new phrases to be added into the learned phrase \n",
    "    # list as specified by the calling fuinction\n",
    "    maxRewrite=maxPhrasesToAdd\n",
    "\n",
    "    # If the remaining number of proposed ngram phrases is less \n",
    "    # than the max allowed, then reset maxRewrite to the size of \n",
    "    # the proposed ngram phrases list\n",
    "    numNgrams = len(rankedNgrams)\n",
    "    if numNgrams < maxRewrite:\n",
    "        maxRewrite = numNgrams\n",
    "    \n",
    "    # Create empty hash tables to keep track of phrase overlap conflicts\n",
    "    leftConflictHash = {}\n",
    "    rightConflictHash = {}\n",
    "    \n",
    "    # Create an empty hash table collecting the set of rewrite rules\n",
    "    # to be applied during this iteration of phrase learning\n",
    "    ngramRewriteHash = {}\n",
    "    \n",
    "    # Precompile the regex for finding spaces in ngram phrases\n",
    "    regexSpace = re.compile(' ')\n",
    "\n",
    "    # Initialize some bookkeeping variables\n",
    "    numLines = len(textData)\n",
    "    numPhrasesAdded = 0\n",
    "    numConsidered = 0\n",
    "    lastSkippedNgram = \"\"\n",
    "    lastAddedNgram = \"\"\n",
    "  \n",
    "    # Collect list up to maxRewrite ngram phrase rewrites\n",
    "    stop = False\n",
    "    index = 0\n",
    "    while not stop:\n",
    "\n",
    "        # Get the next phrase to consider adding to the phrase list\n",
    "        inputNgram = rankedNgrams[index]\n",
    "\n",
    "        # Create the output compound word version of the phrase\n",
    "        # The extra space is added to make the regex rewrite easier\n",
    "        outputNgram = \" \" + regexSpace.sub(\"_\",inputNgram)\n",
    "\n",
    "        # Count the total number of words in the proposed phrase\n",
    "        numWords = len(outputNgram.split(\"_\"))\n",
    "\n",
    "        # Only add phrases that don't exceed the max phrase length\n",
    "        if (numWords <= maxPhraseLength):\n",
    "    \n",
    "            # Keep count of phrases considered for inclusion during this iteration\n",
    "            numConsidered += 1\n",
    "\n",
    "            # Extract the left and right words in the phrase to use\n",
    "            # in checks for phrase overlap conflicts\n",
    "            ngramArray = inputNgram.split()\n",
    "            leftWord = ngramArray[0]\n",
    "            rightWord = ngramArray[len(ngramArray)-1]\n",
    "\n",
    "            # Skip any ngram phrases that conflict with earlier phrases added\n",
    "            # These ngram phrases will be reconsidered in the next iteration\n",
    "            if (leftWord in leftConflictHash) or (rightWord in rightConflictHash): \n",
    "                if verbose: \n",
    "                    print (\"(%d) Skipping (context conflict): %s\" % (numConsidered,inputNgram))\n",
    "                lastSkippedNgram = inputNgram\n",
    "                \n",
    "            # If no conflict exists then add this phrase into the list of phrase rewrites     \n",
    "            else: \n",
    "                if verbose:\n",
    "                    print (\"(%d) Adding: %s\" % (numConsidered,inputNgram))\n",
    "                ngramRewriteHash[\" \" + inputNgram] = outputNgram\n",
    "                learnedPhrases.append(inputNgram) \n",
    "                lastAddedNgram = inputNgram\n",
    "                numPhrasesAdded += 1\n",
    "            \n",
    "            # Keep track of all context words that might conflict with upcoming\n",
    "            # propose phrases (even when phrases are skipped instead of added)\n",
    "            leftConflictHash[rightWord] = 1\n",
    "            rightConflictHash[leftWord] = 1\n",
    "\n",
    "            # Stop when we've considered the maximum number of phrases per iteration\n",
    "            if ( numConsidered >= maxRewrite ):\n",
    "                stop = True\n",
    "            \n",
    "        # Increment to next phrase\n",
    "        index += 1\n",
    "    \n",
    "        # Stop if we've reached the end of the ranked ngram list\n",
    "        if index >= len(rankedNgrams):\n",
    "            stop = True\n",
    "\n",
    "    # Now do the phrase rewrites over the entire set of text data\n",
    "    if numPhrasesAdded == 1:\n",
    "        # If only one phrase to add use a single regex rule to do this phrase rewrite        \n",
    "        inputNgram = \" \" + lastAddedNgram\n",
    "        outputNgram = ngramRewriteHash[inputNgram]\n",
    "        regexNgram = re.compile (r'%s(?= )' % re.escape(inputNgram)) \n",
    "        # Apply the regex over the full data set\n",
    "        for j in range(0,numLines):\n",
    "            textData[j] = regexNgram.sub(outputNgram, textData[j])\n",
    "    elif numPhrasesAdded > 1:\n",
    "        # Compile a single regex rule from the collected set of phrase rewrites for this iteration\n",
    "        ngramRegex = re.compile(r'%s(?= )' % \"|\".join(map(re.escape, ngramRewriteHash.keys())))\n",
    "        # Apply the regex over the full data set\n",
    "        for i in range(0,len(textData)):\n",
    "            # The regex substituion looks up the output string rewrite  \n",
    "            # in the hash table for each matched input phrase regex\n",
    "            textData[i] = ngramRegex.sub(lambda mo: ngramRewriteHash[mo.string[mo.start():mo.end()]], textData[i]) \n",
    "      \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the full iterative phrase learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ApplyPhraseLearning(textData,learnedPhrases,learningSettings):\n",
    "    \n",
    "    stop = 0\n",
    "    iterNum = 0\n",
    "\n",
    "    # Get the learning parameters from the structue passed in by thee calling function\n",
    "    maxNumPhrases = learningSettings.maxNumPhrases\n",
    "    maxPhraseLength = learningSettings.maxPhraseLength\n",
    "    functionwordHash = learningSettings.functionwordHash\n",
    "    blacklistHash = learningSettings.blacklistHash\n",
    "    verbose = learningSettings.verbose\n",
    "    minCount = learningSettings.minInstanceCount\n",
    "    \n",
    "    # Start timing the process\n",
    "    functionStartTime = time.clock()\n",
    "    \n",
    "    numPhrasesLearned = len(learnedPhrases)\n",
    "    print (\"Start phrase learning with %d phrases of %d phrases learned\" % (numPhrasesLearned,maxNumPhrases))\n",
    "\n",
    "    while not stop:\n",
    "        iterNum += 1\n",
    "                \n",
    "        # Start timing this iteration\n",
    "        startTime = time.clock()\n",
    " \n",
    "        # Collect ngram stats\n",
    "        ngramStats = ComputeNgramStats(textData,functionwordHash,blacklistHash)\n",
    "\n",
    "        # Rank ngrams\n",
    "        rankedNgrams = RankNgrams(ngramStats,functionwordHash,minCount)\n",
    "        \n",
    "        # Incorporate top ranked phrases into phrase list\n",
    "        # and rewrite the text to use these phrases\n",
    "        maxPhrasesToAdd = maxNumPhrases - numPhrasesLearned\n",
    "        if maxPhrasesToAdd > learningSettings.maxPhrasesPerIter:\n",
    "            maxPhrasesToAdd = learningSettings.maxPhrasesPerIter\n",
    "        ApplyPhraseRewrites(rankedNgrams,textData,learnedPhrases,maxPhrasesToAdd,maxPhraseLength,verbose)\n",
    "        numPhrasesAdded = len(learnedPhrases) - numPhrasesLearned\n",
    "\n",
    "        # Garbage collect\n",
    "        ngramStats = None\n",
    "        rankedNgrams = None\n",
    "        gc.collect();\n",
    "               \n",
    "        elapsedTime = time.clock() - startTime\n",
    "\n",
    "        numPhrasesLearned = len(learnedPhrases)\n",
    "        print (\"Iteration %d: Added %d new phrases in %.2f seconds (Learned %d of max %d)\" % \n",
    "               (iterNum,numPhrasesAdded,elapsedTime,numPhrasesLearned,maxNumPhrases))\n",
    "        \n",
    "        if numPhrasesAdded >= maxPhrasesToAdd or numPhrasesAdded == 0:\n",
    "            stop = 1\n",
    "        \n",
    "    # Remove the space padding at the start and end of each line\n",
    "    regexSpacePadding = re.compile('^ +| +$')\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i] = regexSpacePadding.sub(\"\",textData[i])\n",
    "    \n",
    "    gc.collect()\n",
    " \n",
    "    elapsedTime = time.clock() - functionStartTime\n",
    "    elapsedTimeHours = elapsedTime/3600.0;\n",
    "    print (\"*** Phrase learning completed in %.2f hours ***\" % elapsedTimeHours) \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a structure defining the settings and word lists used during the phrase learning\n",
    "learningSettings = namedtuple('learningSettings',['maxNumPhrases','maxPhrasesPerIter',\n",
    "                                                  'maxPhraseLength','minInstanceCount'\n",
    "                                                  'functionwordHash','blacklistHash','verbose'])\n",
    "\n",
    "# If true it prints out the learned phrases to stdout buffer\n",
    "# while its learning. This will generate a lot of text to stdout, \n",
    "# so best to turn this off except for testing and debugging\n",
    "learningSettings.verbose = False\n",
    "\n",
    "# Maximium number of phrases to learn\n",
    "# If you want to test the code out quickly then set this to a small\n",
    "# value (e.g. 100) and set verbose to true when running the quick test\n",
    "learningSettings.maxNumPhrases = 200\n",
    "\n",
    "# Maximum number of phrases to learn per iteration \n",
    "# Increasing this number may speed up processing but will affect the ordering of the phrases \n",
    "# learned and good phrases could be by-passed if the maxNumPhrases is set to a small number\n",
    "learningSettings.maxPhrasesPerIter = 50\n",
    "\n",
    "# Maximum number of words allowed in the learned phrases \n",
    "learningSettings.maxPhraseLength = 7\n",
    "\n",
    "# Minimum number of times a phrase must occur in the data to \n",
    "# be considered during the phrase learning process\n",
    "learningSettings.minInstanceCount = 5\n",
    "\n",
    "# This is a precreated hash table containing the list \n",
    "# of function words used during phrase learning\n",
    "learningSettings.functionwordHash = functionwordHash\n",
    "\n",
    "# This is a precreated hash table containing the list \n",
    "# of black list words to be ignored during phrase learning\n",
    "learningSettings.blacklistHash = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start phrase learning with 0 phrases of 200 phrases learned\n",
      "Iteration 1: Added 42 new phrases in 1.14 seconds (Learned 42 of max 200)\n",
      "Iteration 2: Added 35 new phrases in 1.08 seconds (Learned 77 of max 200)\n",
      "Iteration 3: Added 32 new phrases in 1.07 seconds (Learned 109 of max 200)\n",
      "Iteration 4: Added 34 new phrases in 1.08 seconds (Learned 143 of max 200)\n",
      "Iteration 5: Added 31 new phrases in 1.06 seconds (Learned 174 of max 200)\n",
      "Iteration 6: Added 11 new phrases in 1.00 seconds (Learned 185 of max 200)\n",
      "Iteration 7: Added 3 new phrases in 0.98 seconds (Learned 188 of max 200)\n",
      "Iteration 8: Added 4 new phrases in 0.97 seconds (Learned 192 of max 200)\n",
      "Iteration 9: Added 1 new phrases in 0.97 seconds (Learned 193 of max 200)\n",
      "Iteration 10: Added 1 new phrases in 0.96 seconds (Learned 194 of max 200)\n",
      "Iteration 11: Added 1 new phrases in 0.96 seconds (Learned 195 of max 200)\n",
      "Iteration 12: Added 1 new phrases in 0.96 seconds (Learned 196 of max 200)\n",
      "Iteration 13: Added 1 new phrases in 1.04 seconds (Learned 197 of max 200)\n",
      "Iteration 14: Added 1 new phrases in 1.01 seconds (Learned 198 of max 200)\n",
      "Iteration 15: Added 1 new phrases in 0.96 seconds (Learned 199 of max 200)\n",
      "Iteration 16: Added 1 new phrases in 1.00 seconds (Learned 200 of max 200)\n",
      "*** Phrase learning completed in 0.00 hours ***\n"
     ]
    }
   ],
   "source": [
    "###### Questions:\n",
    "# Initialize an empty list of learned phrases\n",
    "# If you have completed a partial run of phrase learning\n",
    "# and want to add more phrases, you can use the pre-learned \n",
    "# phrases as a starting point instead and the new phrases\n",
    "# will be appended to the list\n",
    "learnedPhrasesQ = []\n",
    "\n",
    "# Create a copy of the original text data that will be used during learning\n",
    "# The copy is needed because the algorithm does in-place replacement of learned\n",
    "# phrases directly on the text data structure it is provided\n",
    "phraseTextDataQ = []\n",
    "for textLine in CleanedTrainQ['LowercaseText']:\n",
    "    phraseTextDataQ.append(' ' + textLine + ' ')\n",
    "\n",
    "# Run the phrase learning algorithm\n",
    "ApplyPhraseLearning(phraseTextDataQ,learnedPhrasesQ,learningSettings)\n",
    "\n",
    "# Add text with learned phrases back into data frame\n",
    "CleanedTrainQ['TextWithPhrases'] = phraseTextDataQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the Learned Phrases to Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ApplyPhraseRewritesInPlace(textFrame, textColumnName, phraseRules):\n",
    "        \n",
    "    # Get text data column from frame\n",
    "    textData = textFrame[textColumnName]\n",
    "    numLines = len(textData)\n",
    "    \n",
    "    # initial a list to store output text\n",
    "    textOutput = [None] * numLines\n",
    "    \n",
    "    # Add leading and trailing spaces to make regex matching easier\n",
    "    for i in range(0,numLines):\n",
    "        textOutput[i] = \" \" + textData[i] + \" \"  \n",
    "\n",
    "    # Make sure we have phrase to add\n",
    "    numPhraseRules = len(phraseRules)\n",
    "    if numPhraseRules == 0: \n",
    "        print (\"Warning: phrase rule lise is empty - no phrases being applied to text data\")\n",
    "        return\n",
    "\n",
    "    # Precompile the regex for finding spaces in ngram phrases\n",
    "    regexSpace = re.compile(' ')\n",
    "   \n",
    "    # Initialize some bookkeeping variables\n",
    "\n",
    "    # Iterate through full set of phrases to find sets of \n",
    "    # non-conflicting phrases that can be apply simultaneously\n",
    "    index = 0\n",
    "    outerStop = False\n",
    "    while not outerStop:\n",
    "       \n",
    "        # Create empty hash tables to keep track of phrase overlap conflicts\n",
    "        leftConflictHash = {}\n",
    "        rightConflictHash = {}\n",
    "        prevConflictHash = {}\n",
    "    \n",
    "        # Create an empty hash table collecting the next set of rewrite rules\n",
    "        # to be applied during this iteration of phrase rewriting\n",
    "        phraseRewriteHash = {}\n",
    "    \n",
    "        # Progress through phrases until the next conflicting phrase is found\n",
    "        innerStop = 0\n",
    "        numPhrasesAdded = 0\n",
    "        while not innerStop:\n",
    "        \n",
    "            # Get the next phrase to consider adding to the phrase list\n",
    "            nextPhrase = phraseRules[index]            \n",
    "            \n",
    "            # Extract the left and right sides of the phrase to use\n",
    "            # in checks for phrase overlap conflicts\n",
    "            ngramArray = nextPhrase.split()\n",
    "            leftWord = ngramArray[0]\n",
    "            rightWord = ngramArray[len(ngramArray)-1] \n",
    "\n",
    "            # Stop if we reach any phrases that conflicts with earlier phrases in this iteration\n",
    "            # These ngram phrases will be reconsidered in the next iteration\n",
    "            if ((leftWord in leftConflictHash) or (rightWord in rightConflictHash) \n",
    "                or (leftWord in prevConflictHash) or (rightWord in prevConflictHash)): \n",
    "                innerStop = True\n",
    "                \n",
    "            # If no conflict exists then add this phrase into the list of phrase rewrites     \n",
    "            else: \n",
    "                # Create the output compound word version of the phrase\n",
    "                                \n",
    "                outputPhrase = regexSpace.sub(\"_\",nextPhrase);\n",
    "                \n",
    "                # Keep track of all context words that might conflict with upcoming\n",
    "                # propose phrases (even when phrases are skipped instead of added)\n",
    "                leftConflictHash[rightWord] = 1\n",
    "                rightConflictHash[leftWord] = 1\n",
    "                prevConflictHash[outputPhrase] = 1           \n",
    "                \n",
    "                # Add extra space to input an output versions of the current phrase \n",
    "                # to make the regex rewrite easier\n",
    "                outputPhrase = \" \" + outputPhrase\n",
    "                lastAddedPhrase = \" \" + nextPhrase\n",
    "                \n",
    "                # Add the phrase to the rewrite hash\n",
    "                phraseRewriteHash[lastAddedPhrase] = outputPhrase\n",
    "                  \n",
    "                # Increment to next phrase\n",
    "                index += 1\n",
    "                numPhrasesAdded  += 1\n",
    "    \n",
    "                # Stop if we've reached the end of the phrases list\n",
    "                if index >= numPhraseRules:\n",
    "                    innerStop = True\n",
    "                    outerStop = True\n",
    "                    \n",
    "        # Now do the phrase rewrites over the entire set of text data\n",
    "        if numPhrasesAdded == 1:\n",
    "        \n",
    "            # If only one phrase to add use a single regex rule to do this phrase rewrite        \n",
    "            outputPhrase = phraseRewriteHash[lastAddedPhrase]\n",
    "            regexPhrase = re.compile (r'%s(?= )' % re.escape(lastAddedPhrase)) \n",
    "        \n",
    "            # Apply the regex over the full data set\n",
    "            for j in range(0,numLines):\n",
    "                textOutput[j] = regexPhrase.sub(outputPhrase, textOutput[j])\n",
    "       \n",
    "        elif numPhrasesAdded > 1:\n",
    "            # Compile a single regex rule from the collected set of phrase rewrites for this iteration\n",
    "            regexPhrase = re.compile(r'%s(?= )' % \"|\".join(map(re.escape, phraseRewriteHash.keys())))\n",
    "            \n",
    "            # Apply the regex over the full data set\n",
    "            for i in range(0,numLines):\n",
    "                # The regex substituion looks up the output string rewrite  \n",
    "                # in the hash table for each matched input phrase regex\n",
    "                textOutput[i] = regexPhrase.sub(lambda mo: phraseRewriteHash[mo.string[mo.start():mo.end()]], textOutput[i]) \n",
    "    \n",
    "    # Remove the space padding at the start and end of each line\n",
    "    regexSpacePadding = re.compile('^ +| +$')\n",
    "    for i in range(0,len(textOutput)):\n",
    "        textOutput[i] = regexSpacePadding.sub(\"\",textOutput[i])\n",
    "    \n",
    "    return textOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CleanedTestQ['TextWithPhrases'] = ApplyPhraseRewritesInPlace(CleanedTestQ, 'LowercaseText', learnedPhrasesQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocID                              23830543\n",
       "DocLine                                   9\n",
       "CleanedText        it seems that this works\n",
       "LowercaseText      it seems that this works\n",
       "TextWithPhrases    it seems that this works\n",
       "Name: 9, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show an example of cleaned testQ\n",
    "CleanedTestQ.loc[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct the Full Processed Text of Each Document and Put it into a New FrameÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReconstituteDocsFromChunks(textData, idColumnName, textColumnName):\n",
    "    dataOut = []\n",
    "    \n",
    "    currentDoc = \"\";\n",
    "    currentDocID = \"\";\n",
    "    \n",
    "    for i in range(0,len(textData)):\n",
    "        textChunk = textData[textColumnName][i]\n",
    "        docID = textData[idColumnName][i]\n",
    "        if docID != currentDocID:\n",
    "            if currentDocID != \"\":\n",
    "                dataOut.append(currentDoc)\n",
    "            currentDoc = textChunk\n",
    "            currentDocID = docID\n",
    "        else:\n",
    "            currentDoc += \" \" + textChunk\n",
    "    dataOut.append(currentDoc)\n",
    "    \n",
    "    return dataOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainQ['TextWithPhrases'] = ReconstituteDocsFromChunks(CleanedTrainQ, 'DocID', 'TextWithPhrases')\n",
    "testQ['TextWithPhrases'] = ReconstituteDocsFromChunks(CleanedTestQ, 'DocID', 'TextWithPhrases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Vocabulary With Filtering Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CreateVocabForTopicModeling(textData,stopwordHash):\n",
    "\n",
    "    print (\"Counting words\")\n",
    "    numDocs = len(textData) \n",
    "    globalWordCountHash = {} \n",
    "    globalDocCountHash = {} \n",
    "    for textLine in textData:\n",
    "        docWordCountHash = {}\n",
    "        for word in textLine.split():\n",
    "            if word in globalWordCountHash:\n",
    "                globalWordCountHash[word] += 1\n",
    "            else:\n",
    "                globalWordCountHash[word] = 1\n",
    "            if word not in docWordCountHash: \n",
    "                docWordCountHash[word] = 1\n",
    "                if word in globalDocCountHash:\n",
    "                    globalDocCountHash[word] += 1\n",
    "                else:\n",
    "                    globalDocCountHash[word] = 1\n",
    "\n",
    "    minWordCount = 5;\n",
    "    minDocCount = 2;\n",
    "    maxDocFreq = .25;\n",
    "    vocabCount = 0;\n",
    "    vocabHash = {}\n",
    "\n",
    "    excStopword = 0\n",
    "    excNonalphabetic = 0\n",
    "    excMinwordcount = 0\n",
    "    excNotindochash = 0\n",
    "    excMindoccount = 0\n",
    "    excMaxdocfreq =0\n",
    "\n",
    "    print (\"Building vocab\")\n",
    "    for word in globalWordCountHash.keys():\n",
    "        # Test vocabulary exclusion criteria for each word\n",
    "        if ( word in stopwordHash ):\n",
    "            excStopword += 1\n",
    "        elif ( not re.search(r'[a-zA-Z]', word, 0) ):\n",
    "            excNonalphabetic += 1\n",
    "        elif ( globalWordCountHash[word] < minWordCount ):\n",
    "            excMinwordcount += 1\n",
    "        elif ( word not in globalDocCountHash ):\n",
    "            print (\"Warning: Word '%s' not in doc count hash\") % (word)\n",
    "            excNotindochash += 1\n",
    "        elif ( globalDocCountHash[word] < minDocCount ):\n",
    "            excMindoccount += 1\n",
    "        elif ( float(globalDocCountHash[word])/float(numDocs) > maxDocFreq ):\n",
    "            excMaxdocfreq += 1\n",
    "        else:\n",
    "            # Add word to vocab\n",
    "            vocabHash[word]= globalWordCountHash[word];\n",
    "            vocabCount += 1 \n",
    "    print (\"Excluded %d stop words\" % (excStopword))       \n",
    "    print (\"Excluded %d non-alphabetic words\" % (excNonalphabetic))  \n",
    "    print (\"Excluded %d words below word count threshold\" % (excMinwordcount)) \n",
    "    print (\"Excluded %d words below doc count threshold\" % (excMindoccount))\n",
    "    print (\"Excluded %d words above max doc frequency\" % (excMaxdocfreq)) \n",
    "    print (\"Final Vocab Size: %d words\" % vocabCount)\n",
    "            \n",
    "    return vocabHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words\n",
      "Building vocab\n",
      "Excluded 307 stop words\n",
      "Excluded 911 non-alphabetic words\n",
      "Excluded 15276 words below word count threshold\n",
      "Excluded 142 words below doc count threshold\n",
      "Excluded 3 words above max doc frequency\n",
      "Final Vocab Size: 3116 words\n"
     ]
    }
   ],
   "source": [
    "vocabHashQ = CreateVocabForTopicModeling(trainQ['TextWithPhrases'],functionwordHash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Text with Learned Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start by tokenizing the full text string string for each document into list of tokens.\n",
    "# any token that is in not in the pre-defined set of acceptable vocabulary words is execluded.\n",
    "def TokenizeText(textData,vocabHash):\n",
    "    tokenizedText = ''\n",
    "    for token in textData.split():\n",
    "        if token in vocabHash:\n",
    "            tokenizedText += (token.strip() + ',')\n",
    "    return tokenizedText.strip(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainQ['Tokens'] = trainQ['TextWithPhrases'].apply(lambda x: TokenizeText(x, vocabHashQ))\n",
    "testQ['Tokens'] = testQ['TextWithPhrases'].apply(lambda x: TokenizeText(x, vocabHashQ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'self-closing,script,tags,work,reason,browsers,correctly,recognize,recognized,break,concept,xhtml,support,note,statement,correct,all,ie'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example of tokenized text in training set.\n",
    "trainQ['Tokens'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Selecting the right set of features is very critical for the model training. Therefore, we're going to show you several feature extraction approaches that have been testied to perform well in the text classification use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get Token to ID mapping: {Token: tokenId}\n",
    "def tokensToIds(tokens, featureHash):\n",
    "    token2IdHash = {}\n",
    "    for i in range(len(tokens)):\n",
    "        tokenList = tokens.iloc[i].split(',')\n",
    "        if featureHash is None:\n",
    "            for t in tokenList:\n",
    "                if t not in token2IdHash.keys():\n",
    "                    token2IdHash[t] = len(token2IdHash)\n",
    "        else:\n",
    "            for t in tokenList:\n",
    "                if t not in token2IdHash.keys() and t in list(featureHash.keys()):\n",
    "                    token2IdHash[t] = len(token2IdHash)\n",
    "            \n",
    "    return token2IdHash\n",
    "\n",
    "##########################################\n",
    "\n",
    "def countMatrix(frame, token2IdHash, labelColumnName=None, uniqueLabel=None):\n",
    "    # create am empty matrix with the shape of:\n",
    "    # num_row = num of unique tokens\n",
    "    # num_column = num of unique answerIds (N_wA) or num of questions in testQ (tfMatrix)\n",
    "    # rowIdx = token2IdHash.values()\n",
    "    # colIdx = index of uniqueClass (N_wA) or index of questions in testQ (tfMatrix)\n",
    "    num_row = len(token2IdHash)\n",
    "    if uniqueLabel is not None:  # get N_wA\n",
    "        num_column = len(uniqueLabel)\n",
    "    else:\n",
    "        num_column = len(frame)\n",
    "    countMatrix = np.zeros(shape=(num_row, num_column))\n",
    "\n",
    "    # loop through each question in the frame to fill in the countMatrix with corresponding counts\n",
    "    for i in range(len(frame)):\n",
    "        tokens = frame['Tokens'].iloc[i].split(',')\n",
    "        if uniqueLabel is not None:   # get N_wA\n",
    "            label = frame[labelColumnName].iloc[i]\n",
    "            colIdx = uniqueLabel.index(label)\n",
    "        else:\n",
    "            colIdx = i\n",
    "            \n",
    "        for t in tokens:\n",
    "            if t in token2IdHash.keys():\n",
    "                rowIdx = token2IdHash[t]\n",
    "                countMatrix[rowIdx, colIdx] += 1\n",
    "\n",
    "    return countMatrix\n",
    "\n",
    "##############################################\n",
    "\n",
    "# calculate P(A): [P_A1, P_A2, ...]\n",
    "def priorProbabilityAnswer(answerIds, uniqueLabel): \n",
    "    P_A = []\n",
    "    # convert a pandas series to a list\n",
    "    answerIds = list(answerIds)\n",
    "    \n",
    "    for id in uniqueLabel:\n",
    "        P_A.append(answerIds.count(id)/len(answerIds))\n",
    "    return np.array(P_A)\n",
    "\n",
    "##############################################\n",
    "\n",
    "# calculate P(A|w)\n",
    "def posterioriProb(N_wAInit, P_A, uniqueLabel):\n",
    "    # N_A is the total number of answers\n",
    "    N_A = len(uniqueLabel)\n",
    "    # N_w is the total number of times w appears over all documents \n",
    "    # rowSum of count matrix (N_wAInit)\n",
    "    N_wInit = np.sum(N_wAInit, axis = 1)\n",
    "    # P(A|w) = (N_w|A + N_A * P(A))/(N_w + N_A)\n",
    "    N = N_wAInit + N_A * P_A\n",
    "    D = N_wInit + N_A\n",
    "    P_Aw = np.divide(N.T, D).T    \n",
    "    \n",
    "    return P_Aw\n",
    "\n",
    "##############################################\n",
    "\n",
    "# select the top N tokens w which maximize P(A|w) for each A.\n",
    "# get FeatureHash: {token: 1}\n",
    "def feature_selection(P_Aw, token2IdHashInit, topN):\n",
    "    featureHash = {}\n",
    "    # for each answer A, sort tokens w by P(A|w)\n",
    "    sortedIdxMatrix = np.argsort(P_Aw, axis=0)[::-1]\n",
    "    # select top N tokens for each answer A\n",
    "    topMatrix = sortedIdxMatrix[0:topN, :]\n",
    "    # for each token w in topMatrix, add w to FeatureHash if it has not already been included\n",
    "    topTokenIdList = np.reshape(topMatrix, topMatrix.shape[0] * topMatrix.shape[1])\n",
    "    # get ID to Token mapping: {tokenId: Token}\n",
    "    Id2TokenHashInit = {y:x for x, y in token2IdHashInit.items()}\n",
    "    \n",
    "    for tokenId in topTokenIdList:\n",
    "        token = Id2TokenHashInit[tokenId]\n",
    "        if token not in featureHash.keys():\n",
    "            featureHash[token] = 1\n",
    "    return featureHash\n",
    "\n",
    "##############################################\n",
    "\n",
    "def featureWeights(N_wA, alpha):\n",
    "    # N_w is the total number of times w appears over all documents \n",
    "    # rowSum of count matrix (N_wA)\n",
    "    N_w = np.sum(N_wA, axis = 1)\n",
    "    # N_W is the total count of all words\n",
    "    N_W = np.sum(N_wA)\n",
    "    # N_V is the count of unique words in the vocabulary\n",
    "    N_V = N_wA.shape[0]\n",
    "    # P(w) = (N_w + 1*alpha) / (N_W +N_V*alpha)\n",
    "    N2 = N_w + 1 * alpha\n",
    "    D2 = N_W + alpha * N_V\n",
    "    P_w = N2/D2\n",
    "\n",
    "    return P_w\n",
    "\n",
    "##############################################\n",
    "\n",
    "def wordProbabilityInAnswer(N_wA, P_w, beta):\n",
    "    # N_V is the count of unique words in the vocabulary\n",
    "    N_V = N_wA.shape[0]\n",
    "    # N_WA is the total count of all words in questions on answer A \n",
    "    # colSum of count matrix (N_wA)\n",
    "    N_WA = np.sum(N_wA, axis=0)\n",
    "    # P(w|A) = (N_w|A + beta N_V P(w))/(N_W|A + beta * N_V)\n",
    "    N = (N_wA.T + beta * N_V * P_w).T\n",
    "    D = N_WA + beta * N_V\n",
    "    P_wA = N / D\n",
    "    \n",
    "    return P_wA\n",
    "\n",
    "################################################\n",
    "\n",
    "def wordProbabilityNotinAnswer(N_wA, P_w, beta):\n",
    "    # N_V is the count of unique words in the vocabulary\n",
    "    N_V = N_wA.shape[0]\n",
    "    # N_wNotA is the count of w over all documents but not on answer A\n",
    "    # N_wNotA = N_w - N_wA\n",
    "    N_w = np.sum(N_wA, axis = 1)\n",
    "    N_wNotA = (N_w - N_wA.T).T\n",
    "    # N_WNotA is the count of all words over all documents but not on answer A\n",
    "    # N_WNotA = N_W - N_WA\n",
    "    N_W = np.sum(N_wA)\n",
    "    N_WA = np.sum(N_wA, axis=0)\n",
    "    N_WNotA = N_W - N_WA\n",
    "    # P(w|NotA) = (N_w|NotA + beta * N_V * P(w))/(N_W|NotA + beta * N_V)\n",
    "    N = (N_wNotA.T + beta * N_V * P_w).T\n",
    "    D = N_WNotA + beta * N_V\n",
    "    P_wNotA = N / D\n",
    "    \n",
    "    return P_wNotA\n",
    "\n",
    "#################################################\n",
    "\n",
    "def normalizeTF(frame, token2IdHash):\n",
    "    \n",
    "    N_wQ = countMatrix(frame, token2IdHash)\n",
    "    N_WQ = np.sum(N_wQ, axis=0)\n",
    "    \n",
    "    # find the index where N_WQ is zero\n",
    "    zeroIdx = np.where(N_WQ == 0)[0]\n",
    "    \n",
    "    # if N_WQ is zero, then the x_w for that particular question would be zero.\n",
    "    # for a simple calculation, we convert the N_WQ to 1 in those cases so the demoninator is not zero. \n",
    "    if len(zeroIdx) > 0:\n",
    "        N_WQ[zeroIdx] = 1\n",
    "    \n",
    "    # x_w = P_wd = count(w)/sum(count(i in V))\n",
    "    x_w = N_wQ / N_WQ\n",
    "    \n",
    "    return x_w\n",
    "\n",
    "#####################################################\n",
    "\n",
    "def getIDF(N_wQ):\n",
    "    # N is total number of documents in the corpus\n",
    "    # N_V is the number of tokens in the vocabulary\n",
    "    N_V, N = N_wQ.shape\n",
    "    # D is the number of documents where the token w appears\n",
    "    D = np.zeros(shape=(0, N_V))\n",
    "    for i in range(N_V):\n",
    "        D = np.append(D, len(np.nonzero(N_wQ[i, ])[0]))\n",
    "    return np.log(N/D)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "def softmax(scores2D):\n",
    "    # input: scores from different models\n",
    "    # row: test example\n",
    "    # column: label\n",
    "    return np.exp(scores2D)/np.sum(np.exp(scores2D), axis=1)[:, None]\n",
    "\n",
    "#####################################################\n",
    "\n",
    "# train one-vs-rest classifier using NB scores as features.\n",
    "def ovrClassifier(trainLabels, x_wTrain, x_wTest, NBWeights, clf, ratio):\n",
    "    uniqueLabel = np.unique(trainLabels)\n",
    "    dummyLabels = pd.get_dummies(trainLabels)\n",
    "    numTest = x_wTest.shape[1]\n",
    "    Y_test_prob = np.zeros(shape=(numTest, len(uniqueLabel)))\n",
    "\n",
    "    for i in range(len(uniqueLabel)):\n",
    "        X_train_all, Y_train_all = x_wTrain.T * NBWeights[:, i], dummyLabels.iloc[:, i]\n",
    "        X_test = x_wTest.T * NBWeights[:, i]\n",
    "        \n",
    "        # with sample selection.\n",
    "        if ratio is not None:\n",
    "            # ratio = # of Negative/# of Positive\n",
    "            posIdx = np.where(Y_train_all == 1)[0]\n",
    "            negIdx = np.random.choice(np.where(Y_train_all == 0)[0], ratio*len(posIdx))\n",
    "            allIdx = np.concatenate([posIdx, negIdx])\n",
    "            X_train, Y_train = X_train_all[allIdx], Y_train_all.iloc[allIdx]\n",
    "        else: # without sample selection.\n",
    "            X_train, Y_train = X_train_all, Y_train_all\n",
    "            \n",
    "        clf.fit(X_train, Y_train)\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Y_test_prob[:, i] = clf.decision_function(X_test)\n",
    "        else:\n",
    "            Y_test_prob[:, i] = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    return softmax(Y_test_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency and Inverse Document Frequency (TF-IDF) \n",
    "\n",
    "TF-IDF is commonly used as features when training text classification models. \n",
    "\n",
    "Each document d is typically represented by a feature vector x that represents the contents of d. Because different documents can have different lengths, it can be useful to apply L1 normalized feature vector x. Therefore, a normalized __Term Frequency__ matrix can be obtained based on the below formula.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/tf.PNG?token=APoO9tMyEVzqoUJYT9ALcdF3_BryHHEVks5YnIQywA%3D%3D\">\n",
    "\n",
    "Considering all tokens observed in the training questions and answers, we compute their __Inverse Document Frequency__ based on the below formula.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/idf.PNG?token=APoO9qf3WptQgUPVRQJuOt4cobf56-Y3ks5YnhLSwA%3D%3D\">\n",
    "\n",
    "By knowing the __Term Frequency (TF)__ matrix and __Inverse Document Frequency (IDF)__ vector, we can simply compute __TF-IDF__ matrix by multiplying them together.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/tfidf.PNG?token=APoO9gw3rPhLusbG3if65TuVZNAnyqTCks5YnhWPwA%3D%3D\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token2IdHashInit = tokensToIds(trainQ['Tokens'], featureHash=None)\n",
    "# get unique answerId in ascending order\n",
    "uniqueAnswerId = list(np.unique(trainQ['AnswerId']))\n",
    "\n",
    "\n",
    "###### TF-IDF #######\n",
    "N_wQ = countMatrix(trainQ, token2IdHashInit)\n",
    "idf = getIDF(N_wQ)\n",
    "\n",
    "x_wTest = normalizeTF(testQ, token2IdHashInit)\n",
    "x_wTrain = normalizeTF(trainQ, token2IdHashInit)\n",
    "\n",
    "tfidfTest = (x_wTest.T * idf).T\n",
    "tfidfTrain = (x_wTrain.T * idf).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Scores\n",
    "\n",
    "Besides using the IDF as the word weighting mechnism, a hypothesis testing likelihood ratio approach is also implemented here. \n",
    "\n",
    "In this approach, the word weights are associated with the answer classes and are calculated using the below formula.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/NB_weight.PNG?token=APoO9s-2DejCvW03RgK6zXgiXX6UT5WWks5YnjiRwA%3D%3D\">\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/probability_function.PNG?token=APoO9rWEZ1g_OgvWT_pleQlhT2DEFw3tks5YnIHzwA%3D%3D\">\n",
    "\n",
    "By knowing the __Term Frequency (TF)__ matrix and __Weight__ vector for each class, we can simply compute __Naive Bayes Scores__ matrix for each class by multiplying them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### Naive Bayes Scores #####\n",
    "# calculate the count matrix of all training questions.\n",
    "N_wAInit = countMatrix(trainQ, token2IdHashInit, 'AnswerId', uniqueAnswerId)\n",
    "\n",
    "P_A = priorProbabilityAnswer(trainQ['AnswerId'], uniqueAnswerId)\n",
    "P_Aw = posterioriProb(N_wAInit, P_A, uniqueAnswerId)\n",
    "\n",
    "featureHash = feature_selection(P_Aw, token2IdHashInit, topN=20)\n",
    "token2IdHash = tokensToIds(trainQ['Tokens'], featureHash=featureHash)\n",
    "\n",
    "N_wA = countMatrix(trainQ, token2IdHash, 'AnswerId', uniqueAnswerId)\n",
    "\n",
    "alpha = 0.0001\n",
    "P_w = featureWeights(N_wA, alpha)\n",
    "\n",
    "beta = 0.0001\n",
    "P_wA = wordProbabilityInAnswer(N_wA, P_w, beta)\n",
    "P_wNotA = wordProbabilityNotinAnswer(N_wA, P_w, beta)\n",
    "\n",
    "NBWeights = np.log(P_wA / P_wNotA)\n",
    "\n",
    "x_wTrain = normalizeTF(trainQ, token2IdHash)\n",
    "x_wTest = normalizeTF(testQ, token2IdHash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "### Naive Bayes Classifier\n",
    "\n",
    "We implement the _Naive Bayes Classifier_ as described in the paper entitled [\"MCE Training Techniques for Topic Identification of Spoken Audio Documents\"](http://ieeexplore.ieee.org/abstract/document/5742980/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta_A = 0\n",
    "Y_test_prob1 = softmax(-beta_A + np.dot(x_wTest.T, NBWeights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (TF-IDF as features)\n",
    "\n",
    "Traditional SVM training finds a hyperplane which maximally seperates positive and negative training tokens in a vector space. In its standard form, an SVM is a two-class classifier. To create a multi-class SVM for a problem with N_A classes, a one-versus-rest SVM classifier is typically learned for each answer class a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = tfidfTrain.T, np.array(trainQ['AnswerId'])\n",
    "X_test = tfidfTest.T\n",
    "clf = svm.LinearSVC(dual=True, multi_class='ovr', penalty='l2', C=1, loss=\"squared_hinge\", random_state=1)\n",
    "clf.fit(X_train, Y_train)\n",
    "%time Y_test_prob2 = softmax(clf.decision_function(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest (NB Scores as features)\n",
    "\n",
    "Similar to the above one-versus-rest SVM classifier, we also implement a one-versus-rest Random Forest classifier. \n",
    "\n",
    "In each two-class base classifier, we dynamically compute the naive bayes scores for the positive class as the features. Since the number of negative examples is much larger than the number of positive examples, we hold all positive example and randomly select negative examples based on a negative to positive ratio to obtain a balanced training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=250, criterion='entropy', random_state=1)\n",
    "%time Y_test_prob3 = ovrClassifier(trainQ[\"AnswerId\"], x_wTrain, x_wTest, NBWeights, clf, ratio=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model\n",
    "\n",
    "We build an ensemble model by combining the predicted probabilities from three previously trained classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test_prob_aggr = np.mean([Y_test_prob1, Y_test_prob2, Y_test_prob3], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "\n",
    "We use two evaluation matrices to test our model performance. For each question in the test set, we calculate a weighted average of the probabilities obtained from the base classifiers against each answer. Then we rank the answers based on their weighted average to calculate __Average Rank__ and __Top 10 Percentage__ in the Test set using the below formula:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/evaluation.PNG?token=APoO9hyYDFxGc9FRbmIXU3VGv0wdeCaPks5YnIVtwA%3D%3D\">\n",
    "\n",
    "The __Average Rank__ can be interpreted as in average at which position we can find the correct answer among all available answers for a given question. \n",
    "\n",
    "The __Top 10 Percentage__ can be interpreted as how many percentage of the new questions that we can find their correct answers in the first 10 choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort the similarity scores in descending order and map them to the corresponding AnswerId in Answer set\n",
    "def rank(frame, scores, uniqueAnswerId):\n",
    "    frame['SortedAnswers'] = list(np.array(uniqueAnswerId)[np.argsort(-scores, axis=1)])\n",
    "    \n",
    "    rankList = []\n",
    "    for i in range(len(frame)):\n",
    "        rankList.append(np.where(frame['SortedAnswers'].iloc[i] == frame['AnswerId'].iloc[i])[0][0] + 1)\n",
    "    frame['Rank'] = rankList\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testQ = rank(testQ, Y_test_prob_aggr, uniqueAnswerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of questions in test set: 1735\n",
      "Total number of answers: 103\n",
      "Total number of unique features: 1295\n",
      "Average of rank: 5.0\n",
      "Percentage of questions find answers in top 3: 0.688\n"
     ]
    }
   ],
   "source": [
    "print('Total number of questions in test set: ' + str(len(testQ)))\n",
    "print('Total number of answers: ' + str(len(uniqueAnswerId)))\n",
    "print('Total number of unique features: ' + str(len(featureHash)))\n",
    "print('Average of rank: ' + str(np.floor(testQ['Rank'].mean())))\n",
    "print('Percentage of questions find answers in top 3: ' + str(round(len(testQ.query('Rank <= 3'))/len(testQ), 3)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
